{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMS Spam/Ham dataset.\n",
    "1(+6). Проверить, сбалансирован ли датасет (может быть, наблюдений одного класса слишком много?). Какие результаты покажет dummy classifier, который будет всем новым наблюдениям присваивать класс ham? Насколько плохо такое решение для задачи определения спама?\n",
    "Грубое решение - включить в training set только необходимое число наблюдений (примерно поровну spam и ham). \n",
    "Нормализовать тексты и обучить байесовскую модель (bag of words). Проверить, как влияют на результат:\n",
    "1) разная токенизация: в одном случае знаки препинания удалять, в другом — считать их токенами;\n",
    "2) лемматизация (отсутствие лемматизации, стемминг, лемматизация; инструменты можно использовать любые, например, nltk.stem);\n",
    "3) удаление стоп-слов, а также пороги минимальной и максимальной document frequency;\n",
    "4) векторизация документов (CountVectorizer vs. TfIdfVectorizer);\n",
    "5) что-нибудь ещё?\n",
    "При оценке классификатора обратите внимание на TP и FP.\n",
    "\n",
    "Extra: ограничив количество наблюдений ham в обучающей выборке, мы игнорируем довольно много данных. 1) В цикле: случайно выбрать нужное число писем ham и сконструировать сбалансированную выборку, построить классификатор, оценить и записать результат; в итоге результаты усреднить. 2) поможет ли параметр class prior probability?\n",
    "\n",
    "\n",
    "2(+2). Сравнить результаты байесовского классификатора, решающего дерева и RandomForest. Помимо стандартных метрик оценки качества модели, необходимо построить learning curve, ROC-curve, classification report и интерпретировать эти результаты.\n",
    "\n",
    "3(+2). А что, если в качестве предикторов брать не количество вхождений слов, а конструировать специальные признаки? Прежде всего, необходимо разделить таблицу на training set и test set в соотношении 80:20, test set не открывать до этапа оценки модели. С помощью pandas проверить, отличаются ли перечисленные ниже параметры (иможно придумать другие) для разных классов (spam/ham), и собрать матрицу признаков для обучения. Примеры признаков: длина сообщения, количество букв в ВЕРХНЕМ РЕГИСТРЕ, восклицательных знаков, цифр, запятых, каких-то конкретных слов (для этого можно построить частотный словарь по сообщениям каждого класса). Прокомментировать свой выбор. Векторизовать документы и построить классификатор. Оценить модель на проверочной выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                        message\n",
      "label                                                          \n",
      "ham   count                                                4825\n",
      "      unique                                               4516\n",
      "      top                                Sorry, I'll call later\n",
      "      freq                                                   30\n",
      "spam  count                                                 747\n",
      "      unique                                                653\n",
      "      top     Please call our customer service representativ...\n",
      "      freq                                                    4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "\n",
    "messages = pd.read_csv('./smsspamcollection/SMSSpamCollection',\n",
    "                           sep='\\t',\n",
    "                           names=[\"label\", \"message\"])\n",
    "print(messages.groupby('label').describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "видно, что данные чонь несболлансированные,для баллансировки, увеличим коичество spam в 4 раза, просто доавляя по три копии предложения, чтобы не \"резать\" и не сокращать dataset. Потом делим данные на трейн и тест."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                        message\n",
      "label                                                          \n",
      "ham   count                                                4825\n",
      "      unique                                               4516\n",
      "      top                                Sorry, I'll call later\n",
      "      freq                                                   30\n",
      "spam  count                                                2988\n",
      "      unique                                                653\n",
      "      top     Please call our customer service representativ...\n",
      "      freq                                                   16\n"
     ]
    }
   ],
   "source": [
    "#balansing the data\n",
    "balansed_messages_ = messages.append(messages[messages.label == 'spam'], ignore_index=True)\n",
    "balansed_messages = balansed_messages_.append(balansed_messages_[balansed_messages_.label == 'spam'], ignore_index=True)\n",
    "print(balansed_messages.groupby('label').describe())\n",
    "#print(balansed_messages.loc[4680])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#X, y = balansed_messages['message'], balansed_messages['label']\n",
    "train, test = train_test_split(balansed_messages,\n",
    "         test_size=0.2, random_state=42)\n",
    "#print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import re\n",
    "\n",
    "st = LancasterStemmer()\n",
    "\n",
    "lmtzr = WordNetLemmatizer()\n",
    "tknzr = TweetTokenizer()\n",
    "punct = re.compile(',|\\.|;|:|\\?|\\)|\\(|!')\n",
    "non_letters = re.compile('[^a-zA-Z]')\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "def tokenize_np_nl_ds(text):\n",
    "    #no punctuation, no lemmatisation, delete stops\n",
    "    letters_only = non_letters.sub(\" \", text) \n",
    "    words = letters_only.lower().split()                 \n",
    "    meaningful_words = [w for w in words if w not in stops]   \n",
    "    return meaningful_words \n",
    "\n",
    "def tokenize_np_nl(text):\n",
    "    #delete punctuation, no lemmatisation, leave stops\n",
    "    letters_only = non_letters.sub(\" \", text) \n",
    "    words = letters_only.lower().split()                   \n",
    "    return  words \n",
    "\n",
    "def tokenize_p_nl_ds(text):\n",
    "    #leave punct, no lemmatisation, delete stops\n",
    "    txt = tknzr.tokenize(text)\n",
    "    meaningful_words = [w for w in txt if w not in stops]   \n",
    "    return meaningful_words \n",
    "\n",
    "def tokenize_p_nl(text):\n",
    "    #leave punct, no lemmatisation, leave stops\n",
    "    txt = tknzr.tokenize(text)\n",
    "    return  txt\n",
    "\n",
    "def tokenize_p_l_ds(text):\n",
    "    #leave punct, lemmetize, delete stops\n",
    "    txt = tknzr.tokenize(text)\n",
    "    meaningful_words = [lmtzr.lemmatize(w) for w in txt if w not in stops]   \n",
    "    return  meaningful_words \n",
    "\n",
    "def tokenize_p_l(text):\n",
    "    #leave punct, lemmetize, leave stops\n",
    "    txt = tknzr.tokenize(text) \n",
    "    return [lmtzr.lemmatize(w) for w in txt]\n",
    "\n",
    "def tokenize_np_l_ds(text):\n",
    "    #delete punct, lemmetize, delete stops\n",
    "    txt = non_letters.sub(\" \", text) \n",
    "    # Convert to lower case, split into individual words\n",
    "    words = txt.lower().split()\n",
    "    return  [lmtzr.lemmatize(w) for w in words if w not in stops]\n",
    "\n",
    "def tokenize_np_l(text):\n",
    "    #delete punct, lemmetize, delete stops\n",
    "    txt = non_letters.sub(\" \", text) \n",
    "    # Convert to lower case, split into individual words\n",
    "    words = txt.lower().split()\n",
    "    return  [lmtzr.lemmatize(w) for w in words]\n",
    "\n",
    "\n",
    "\n",
    "#same for stemmatisation\n",
    "def tokenize_p_s_ds(text):\n",
    "    #leave punct, lemmetize, delete stops\n",
    "    txt = tknzr.tokenize(text)\n",
    "    meaningful_words = [st.stem(w) for w in txt if w not in stops]   \n",
    "    return  meaningful_words \n",
    "\n",
    "def tokenize_p_s(text):\n",
    "    #leave punct, lemmetize, leave stops\n",
    "    txt = tknzr.tokenize(text) \n",
    "    return [st.stem(w) for w in txt]\n",
    "\n",
    "def tokenize_np_s_ds(text):\n",
    "    #delete punct, lemmetize, delete stops\n",
    "    txt = non_letters.sub(\" \", text) \n",
    "    # Convert to lower case, split into individual words\n",
    "    words = txt.lower().split()\n",
    "    return  [st.stem(w) for w in words if w not in stops]\n",
    "\n",
    "def tokenize_np_s(text):\n",
    "    #delete punct, lemmetize, delete stops\n",
    "    txt = non_letters.sub(\" \", text) \n",
    "    # Convert to lower case, split into individual words\n",
    "    words = txt.lower().split()\n",
    "    return  [st.stem(w) for w in words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For CountVectoriser and for <function tokenize_np_nl_ds at 0x115dd9f28>\n",
      "accuracy_score is:    0.978886756238\n",
      "------------->  tfidf  <------------\n",
      "accuracy_score is:    0.980806142035\n",
      "\n",
      "For CountVectoriser and for <function tokenize_np_nl at 0x11b1eff28>\n",
      "accuracy_score is:    0.982085732566\n",
      "------------->  tfidf  <------------\n",
      "accuracy_score is:    0.984644913628\n",
      "\n",
      "For CountVectoriser and for <function tokenize_p_nl_ds at 0x11b1efea0>\n",
      "accuracy_score is:    0.982725527831\n",
      "------------->  tfidf  <------------\n",
      "accuracy_score is:    0.985284708893\n",
      "\n",
      "For CountVectoriser and for <function tokenize_p_nl at 0x11b1efe18>\n",
      "accuracy_score is:    0.98720409469\n",
      "------------->  tfidf  <------------\n",
      "accuracy_score is:    0.985284708893\n",
      "\n",
      "For CountVectoriser and for <function tokenize_p_l_ds at 0x11b1efd90>\n",
      "accuracy_score is:    0.983365323097\n",
      "------------->  tfidf  <------------\n",
      "accuracy_score is:    0.985284708893\n",
      "\n",
      "For CountVectoriser and for <function tokenize_p_l at 0x11b1efd08>\n",
      "accuracy_score is:    0.987843889955\n",
      "------------->  tfidf  <------------\n",
      "accuracy_score is:    0.985924504159\n",
      "\n",
      "For CountVectoriser and for <function tokenize_np_l_ds at 0x11b1efc80>\n",
      "accuracy_score is:    0.980166346769\n",
      "------------->  tfidf  <------------\n",
      "accuracy_score is:    0.982725527831\n",
      "\n",
      "For CountVectoriser and for <function tokenize_np_l at 0x11b1efbf8>\n",
      "accuracy_score is:    0.980806142035\n",
      "------------->  tfidf  <------------\n",
      "accuracy_score is:    0.982085732566\n",
      "\n",
      "For CountVectoriser and for <function tokenize_p_s_ds at 0x11b1efb70>\n",
      "accuracy_score is:    0.982725527831\n",
      "------------->  tfidf  <------------\n",
      "accuracy_score is:    0.983365323097\n",
      "\n",
      "For CountVectoriser and for <function tokenize_p_s at 0x11b1efae8>\n",
      "accuracy_score is:    0.986564299424\n",
      "------------->  tfidf  <------------\n",
      "accuracy_score is:    0.983365323097\n",
      "\n",
      "For CountVectoriser and for <function tokenize_np_s_ds at 0x11b1efa60>\n",
      "accuracy_score is:    0.974408189379\n",
      "------------->  tfidf  <------------\n",
      "accuracy_score is:    0.976327575176\n",
      "\n",
      "For CountVectoriser and for <function tokenize_np_s at 0x11b1ef9d8>\n",
      "accuracy_score is:    0.980166346769\n",
      "------------->  tfidf  <------------\n",
      "accuracy_score is:    0.978246960972\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "parametrs = [tokenize_np_nl_ds,\n",
    "            tokenize_np_nl,\n",
    "            tokenize_p_nl_ds,\n",
    "            tokenize_p_nl,\n",
    "            tokenize_p_l_ds,\n",
    "            tokenize_p_l,\n",
    "            tokenize_np_l_ds,\n",
    "            tokenize_np_l,\n",
    "            tokenize_p_s_ds,\n",
    "            tokenize_p_s,\n",
    "            tokenize_np_s_ds,\n",
    "            tokenize_np_s,\n",
    "            ]\n",
    "\n",
    "#cv = CountVectorizer()\n",
    "#gs = (cv, [{'analyzer':'word','tokenizer':parametrs}])\n",
    "\n",
    "clf = MultinomialNB()\n",
    "all_params = {}\n",
    "for param in parametrs:\n",
    "    cv = CountVectorizer(analyzer='word', tokenizer=param)\n",
    "    cv.fit_transform(balansed_messages['message'])\n",
    "    transformed_train = cv.transform(train['message'])\n",
    "    transformed_test = cv.transform(test['message'])\n",
    "    clf.fit(transformed_train, train['label'])\n",
    "    print()\n",
    "    print('For CountVectoriser and for '+ str(param))\n",
    "    print('accuracy_score is:    '+ str(accuracy_score(test['label'], clf.predict(transformed_test))))\n",
    "    all_params[accuracy_score( clf.predict(transformed_test), test['label'])] = str(param) + '  +  count'\n",
    "    print('------------->  tfidf  <------------')\n",
    "    \n",
    "    tcv = TfidfVectorizer(analyzer='word', tokenizer=param)\n",
    "    tcv.fit_transform(balansed_messages['message'])\n",
    "    transformed_train = tcv.transform(train['message'])\n",
    "    transformed_test = tcv.transform(test['message'])\n",
    "    clf.fit(transformed_train, train['label'])\n",
    "    print('accuracy_score is:    '+ str(accuracy_score( test['label'], clf.predict(transformed_test))))\n",
    "    all_params[accuracy_score( clf.predict(transformed_test), test['label'])] = str(param) + '  +  tfidf'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best combo:\n",
      "<function tokenize_p_l at 0x11b1efd08>  +  count\n"
     ]
    }
   ],
   "source": [
    "print('The best combo:')\n",
    "print(all_params[sorted(all_params)[len(all_params)-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "как мы видим выше, лучшая обработка - лемматизация без удаления стоп слов и пунктуации. Что, наверное, логично, так как в спам-сообщениях, может быть измененная пунктуация и много стоп слов.\n",
    "\n",
    "Теперь на лучшей обработке построим бейзлайн"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "       spam       0.64      0.61      0.62       973\n",
      "        ham       0.40      0.42      0.41       590\n",
      "\n",
      "avg / total       0.55      0.54      0.54      1563\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "cv = CountVectorizer(analyzer='word', tokenizer=tokenize_p_l)\n",
    "cv.fit_transform(balansed_messages['message'])\n",
    "transformed_train = cv.transform(train['message'])\n",
    "transformed_test = cv.transform(test['message'])\n",
    "\n",
    "\n",
    "dc = DummyClassifier()\n",
    "dc.fit(transformed_train, train['label'])\n",
    "\n",
    "print(classification_report(test['label'], dc.predict(transformed_test), target_names=['spam', 'ham']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно из отчетов ниже, случайный лес выдает идельный результат. Не понятно почему, вряд ли переобучился"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       1.00      0.98      0.99       973\n",
      "       spam       0.97      1.00      0.99       590\n",
      "\n",
      "avg / total       0.99      0.99      0.99      1563\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       1.00      1.00      1.00       973\n",
      "       spam       1.00      1.00      1.00       590\n",
      "\n",
      "avg / total       1.00      1.00      1.00      1563\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#train DecisionTreeClassifier\n",
    "dtc = DecisionTreeClassifier()\n",
    "dtc.fit(transformed_train, train['label'])\n",
    "print(classification_report(test['label'], dtc.predict(transformed_test)))\n",
    "\n",
    "#train RandomForestClassifier\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(transformed_train, train['label'])\n",
    "print(classification_report(test['label'], rfc.predict(transformed_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для рисования lerning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFgUMERGJ5f8D12CXvnPRQB4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ec14198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import learning_curve, ShuffleSplit\n",
    "\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "estimator = MultinomialNB()\n",
    "plot_learning_curve(estimator, 'Naive Bayes', transformed_train.toarray(),\n",
    "                    train['label'], ylim=(0.7, 1.01), cv=cv, n_jobs=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сходится красиво, аккуратно, без разбросов и скачков.\n",
    "а вот с Roc-curve все не так хорошо, почему-то только две точки, да и те на правильные не похожи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2398\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBv0figleBw8Cy7kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11add45f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import scikitplot.plotters as skplt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "clf = MultinomialNB()\n",
    "change = {'spam':1, 'ham':0}\n",
    "\n",
    "y_train = [change[lbl] for lbl in train['label']]\n",
    "\n",
    "clf.fit(transformed_train, y_train)\n",
    "y = [change[lbl] for lbl in train['label']]\n",
    "print(sum(y))\n",
    "score = clf.predict(transformed_train).tolist()\n",
    "#print(score)\n",
    "roc_x = []\n",
    "roc_y = []\n",
    "min_score = min(score)\n",
    "max_score = max(score)\n",
    "thr = np.linspace(min_score, max_score, 30)\n",
    "FP=0\n",
    "TP=0\n",
    "N = sum(y)\n",
    "P = len(y) - N\n",
    "\n",
    "for (i, T) in enumerate(thr):\n",
    "    for i in range(0, len(score)):\n",
    "        if (score[i] > T):\n",
    "            if (y[i]==1):\n",
    "                TP = TP + 1\n",
    "            if (y[i]==0):\n",
    "                FP = FP + 1\n",
    "    roc_x.append(FP/float(N))\n",
    "    roc_y.append(TP/float(P))\n",
    "    FP=0\n",
    "    TP=0\n",
    "\n",
    "plt.scatter(roc_x, roc_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Задание 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем разделение на трейн и тест, которое было сделано еще в начале файла. \n",
    "Построим топ-20 частотных слов для каждого типа сообщений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('to', 1962), ('a', 1185), ('your', 601), ('or', 599), ('the', 574), ('call', 573), ('2', 562), ('for', 554), ('you', 538), ('is', 468), ('Call', 439), ('on', 415), ('have', 399), ('and', 376), ('from', 374), ('ur', 346), ('with', 322), ('of', 313), ('&', 308), ('4', 306)]\n",
      "[('to', 1232), ('I', 1195), ('you', 1189), ('the', 821), ('a', 784), ('and', 596), ('i', 590), ('in', 585), ('is', 513), ('u', 502), ('my', 497), ('me', 440), ('of', 397), ('for', 395), ('that', 328), ('it', 307), ('your', 302), ('have', 278), ('on', 278), ('be', 264)]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "didg = re.compile('[0-9]+')\n",
    "upper_case_word = re.compile('[A-Z][a-z]+')\n",
    "caps_word = re.compile('[A-Z]+')\n",
    "excl = re.compile('!')\n",
    "que = re.compile('\\?')\n",
    "comm = re.compile(',')\n",
    "#mess_words = re.compile('[A-Za-z0-9]+')\n",
    "#data_new = balansed_messages.copy()\n",
    "\n",
    "words_spam = [line.split() for line in train.loc[train['label'] == 'spam']['message'].tolist()]\n",
    "words_ham = [line.split() for line in train.loc[train['label'] == 'ham']['message'].tolist()]\n",
    "words_spam = list(itertools.chain.from_iterable(words_spam))\n",
    "words_ham = list(itertools.chain.from_iterable(words_ham))\n",
    "\n",
    "d_dict_spam = Counter(words_spam)\n",
    "d_dict_ham = Counter(words_ham)\n",
    "print(d_dict_spam.most_common(20))\n",
    "print(d_dict_ham.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "в принципе, частотные списки не особо различаются, поэтому, посмотрев на них, я добавлю только &\n",
    "Выделим 10 параметров для каждого сообщения: \n",
    "количество чисел\n",
    "количество слов с большой буквы\n",
    "количество слов только заглавными буквами\n",
    "количество !\n",
    "количество ?\n",
    "количество ,\n",
    "количество &\n",
    "средняя длина слова\n",
    "длина сообщения\n",
    "длина самого длинного слова\n",
    "Кажется, что спам-сообщения написаны менее грамотно с точки зрения пунктуации, поэтому основной упор не нее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ksenia/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/Ksenia/anaconda/lib/python3.6/site-packages/pandas/core/indexing.py:477: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "import statistics as st\n",
    "\n",
    "amp = re.compile('&')\n",
    "def extreact_features(line): #самое долнное слово, длина сообщения, средняя дляна слов\n",
    "    #print(line)\n",
    "    didg_count = len(didg.findall(line))\n",
    "    upper_case_word_count = len(upper_case_word.findall(line))\n",
    "    caps_word_count = len(caps_word.findall(line))\n",
    "    excl_count = len(excl.findall(line))\n",
    "    que_count = len(que.findall(line))\n",
    "    comm_count = len(comm.findall(line))\n",
    "    amp_count = len(amp.findall(line))\n",
    "    len_words = [len(word[:]) for word in line.split()]\n",
    "    mean_len = st.mean(len_words)\n",
    "    max_len = max(len_words)\n",
    "    len_line = len(line.split())\n",
    "    return [didg_count, upper_case_word_count, caps_word_count,\n",
    "            excl_count, que_count, comm_count, amp_count,\n",
    "            mean_len, max_len, len_line]\n",
    "\n",
    "def make_df(data):\n",
    "    columns = ['didg_count', 'upper_case_word_count', 'caps_word_count',\n",
    "                'excl_count', 'que_count', 'comm_count', 'amp_count',\n",
    "                'mean_len', 'max_len', 'len_line']\n",
    "    for col in columns:\n",
    "        data[col] = pd.Series()\n",
    "    #data = data.add(columns, axis='columns')\n",
    "    \n",
    "    for index, line in data.iterrows():\n",
    "        #ln = pd.Series(line)\n",
    "        #print(index, type(extreact_features(data.loc[index]['message'])), len(extreact_features(data.loc[index]['message'])), len(data.loc[index, columns]))\n",
    "        #for i in range(0,len(columns)):\n",
    "        #    print(extreact_features(data.loc[index]['message']))\n",
    "        #data.set_value(index, columns, extreact_features(data.loc[index]['message']))\n",
    "        #print(data.loc[index, columns])\n",
    "        #print('this is msg ---------------------->')\n",
    "        #print(data.loc[index]['message'])\n",
    "        data.loc[index, columns] = extreact_features(data.loc[index]['message'])\n",
    "    return data\n",
    "\n",
    "#print(train['message'])\n",
    "new_train = make_df(train)\n",
    "evaluate = make_df(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тестовую выборку из предыдущего деления будем рассматривать как evaluation, поэтому для выбора модели нужно еще раз поделить test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "to_train, to_test = train_test_split(train, test_size=0.1, random_state=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "как и для bag of words, сделаем бейзлайн-классификатор и обучим модели рандомного леса, байеса и дерева решений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "columns = ['didg_count', 'upper_case_word_count', 'caps_word_count',\n",
    "            'excl_count', 'que_count', 'comm_count', 'amp_count',\n",
    "            'mean_len', 'max_len', 'len_line']\n",
    "dc = DummyClassifier()\n",
    "dc.fit(to_train[columns], to_train['label'])\n",
    "\n",
    "print(classification_report(to_test['label'], dc.predict(to_test[columns]), target_names=['spam', 'ham']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(to_train[columns], to_train['label'])\n",
    "print(classification_report(to_test['label'], clf.predict(to_test[columns]), target_names=['spam', 'ham']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train DecisionTreeClassifier\n",
    "dtc = DecisionTreeClassifier()\n",
    "dtc.fit(to_train[columns], to_train['label'])\n",
    "print(classification_report(to_test['label'], dtc.predict(to_test[columns]), target_names=['spam', 'ham']))\n",
    "\n",
    "#train RandomForestClassifier\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(to_train[columns], to_train['label'])\n",
    "print(classification_report(to_test['label'], rfc.predict(to_test[columns]), target_names=['spam', 'ham']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "mat = confusion_matrix(to_train_answ, clf.predict(to_train_vec.toarray()))\n",
    "plt.figure()\n",
    "plot_confusion_matrix(mat, classes=['Cartman', 'Kenny' , 'Kyle' , 'Stan'],\n",
    "                      title='Confusion matrix. Logistic Regression')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
