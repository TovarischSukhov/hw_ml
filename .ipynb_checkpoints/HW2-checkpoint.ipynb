{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMS Spam/Ham dataset.\n",
    "1(+6). Проверить, сбалансирован ли датасет (может быть, наблюдений одного класса слишком много?). Какие результаты покажет dummy classifier, который будет всем новым наблюдениям присваивать класс ham? Насколько плохо такое решение для задачи определения спама?\n",
    "Грубое решение - включить в training set только необходимое число наблюдений (примерно поровну spam и ham). \n",
    "Нормализовать тексты и обучить байесовскую модель (bag of words). Проверить, как влияют на результат:\n",
    "1) разная токенизация: в одном случае знаки препинания удалять, в другом — считать их токенами;\n",
    "2) лемматизация (отсутствие лемматизации, стемминг, лемматизация; инструменты можно использовать любые, например, nltk.stem);\n",
    "3) удаление стоп-слов, а также пороги минимальной и максимальной document frequency;\n",
    "4) векторизация документов (CountVectorizer vs. TfIdfVectorizer);\n",
    "5) что-нибудь ещё?\n",
    "При оценке классификатора обратите внимание на TP и FP.\n",
    "\n",
    "Extra: ограничив количество наблюдений ham в обучающей выборке, мы игнорируем довольно много данных. 1) В цикле: случайно выбрать нужное число писем ham и сконструировать сбалансированную выборку, построить классификатор, оценить и записать результат; в итоге результаты усреднить. 2) поможет ли параметр class prior probability?\n",
    "\n",
    "\n",
    "2(+2). Сравнить результаты байесовского классификатора, решающего дерева и RandomForest. Помимо стандартных метрик оценки качества модели, необходимо построить learning curve, ROC-curve, classification report и интерпретировать эти результаты.\n",
    "\n",
    "3(+2). А что, если в качестве предикторов брать не количество вхождений слов, а конструировать специальные признаки? Прежде всего, необходимо разделить таблицу на training set и test set в соотношении 80:20, test set не открывать до этапа оценки модели. С помощью pandas проверить, отличаются ли перечисленные ниже параметры (иможно придумать другие) для разных классов (spam/ham), и собрать матрицу признаков для обучения. Примеры признаков: длина сообщения, количество букв в ВЕРХНЕМ РЕГИСТРЕ, восклицательных знаков, цифр, запятых, каких-то конкретных слов (для этого можно построить частотный словарь по сообщениям каждого класса). Прокомментировать свой выбор. Векторизовать документы и построить классификатор. Оценить модель на проверочной выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                        message\n",
      "label                                                          \n",
      "ham   count                                                4825\n",
      "      unique                                               4516\n",
      "      top                                Sorry, I'll call later\n",
      "      freq                                                   30\n",
      "spam  count                                                 747\n",
      "      unique                                                653\n",
      "      top     Please call our customer service representativ...\n",
      "      freq                                                    4\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "\n",
    "messages = pandas.read_csv('./smsspamcollection/SMSSpamCollection',\n",
    "                           sep='\\t',\n",
    "                           names=[\"label\", \"message\"])\n",
    "print(messages.groupby('label').describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "видно, что данные чонь несболлансированные,для баллансировки, увеличим коичество spam в 4 раза, просто доавляя по три копии предложения, чтобы не \"резать\" и не сокращать dataset. Потом делим данные на трейн и тест."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                        message\n",
      "label                                                          \n",
      "ham   count                                                4825\n",
      "      unique                                               4516\n",
      "      top                                Sorry, I'll call later\n",
      "      freq                                                   30\n",
      "spam  count                                                2988\n",
      "      unique                                                653\n",
      "      top     Please call our customer service representativ...\n",
      "      freq                                                   16\n"
     ]
    }
   ],
   "source": [
    "#balansing the data\n",
    "balansed_messages_ = messages.append(messages[messages.label == 'spam'])\n",
    "balansed_messages = balansed_messages_.append(balansed_messages_[balansed_messages_.label == 'spam'])\n",
    "print(balansed_messages.groupby('label').describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#X, y = balansed_messages['message'], balansed_messages['label']\n",
    "train, test = train_test_split(balansed_messages,\n",
    "         test_size=0.2, random_state=42)\n",
    "#print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import re\n",
    "\n",
    "st = LancasterStemmer()\n",
    "\n",
    "lmtzr = WordNetLemmatizer()\n",
    "tknzr = TweetTokenizer()\n",
    "punct = re.compile(',|\\.|;|:|\\?|\\)|\\(|!')\n",
    "non_letters = re.compile('[^a-zA-Z]')\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "def tokenize_np_nl_ds(text):\n",
    "    #no punctuation, no lemmatisation, delete stops\n",
    "    letters_only = non_letters.sub(\" \", text) \n",
    "    words = letters_only.lower().split()                 \n",
    "    meaningful_words = [w for w in words if w not in stops]   \n",
    "    return meaningful_words \n",
    "\n",
    "def tokenize_np_nl(text):\n",
    "    #delete punctuation, no lemmatisation, leave stops\n",
    "    letters_only = non_letters.sub(\" \", text) \n",
    "    words = letters_only.lower().split()                   \n",
    "    return  words \n",
    "\n",
    "def tokenize_p_nl_ds(text):\n",
    "    #leave punct, no lemmatisation, delete stops\n",
    "    txt = tknzr.tokenize(text)\n",
    "    meaningful_words = [w for w in txt if w not in stops]   \n",
    "    return meaningful_words \n",
    "\n",
    "def tokenize_p_nl(text):\n",
    "    #leave punct, no lemmatisation, leave stops\n",
    "    txt = tknzr.tokenize(text)\n",
    "    return  txt\n",
    "\n",
    "def tokenize_p_l_ds(text):\n",
    "    #leave punct, lemmetize, delete stops\n",
    "    txt = tknzr.tokenize(text)\n",
    "    meaningful_words = [lmtzr.lemmatize(w) for w in txt if w not in stops]   \n",
    "    return  meaningful_words \n",
    "\n",
    "def tokenize_p_l(text):\n",
    "    #leave punct, lemmetize, leave stops\n",
    "    txt = tknzr.tokenize(text) \n",
    "    return [lmtzr.lemmatize(w) for w in txt]\n",
    "\n",
    "def tokenize_np_l_ds(text):\n",
    "    #delete punct, lemmetize, delete stops\n",
    "    txt = non_letters.sub(\" \", text) \n",
    "    # Convert to lower case, split into individual words\n",
    "    words = txt.lower().split()\n",
    "    return  [lmtzr.lemmatize(w) for w in words if w not in stops]\n",
    "\n",
    "def tokenize_np_l(text):\n",
    "    #delete punct, lemmetize, delete stops\n",
    "    txt = non_letters.sub(\" \", text) \n",
    "    # Convert to lower case, split into individual words\n",
    "    words = txt.lower().split()\n",
    "    return  [lmtzr.lemmatize(w) for w in words]\n",
    "\n",
    "\n",
    "\n",
    "#same for stemmatisation\n",
    "def tokenize_p_s_ds(text):\n",
    "    #leave punct, lemmetize, delete stops\n",
    "    txt = tknzr.tokenize(text)\n",
    "    meaningful_words = [st.stem(w) for w in txt if w not in stops]   \n",
    "    return  meaningful_words \n",
    "\n",
    "def tokenize_p_s(text):\n",
    "    #leave punct, lemmetize, leave stops\n",
    "    txt = tknzr.tokenize(text) \n",
    "    return [st.stem(w) for w in txt]\n",
    "\n",
    "def tokenize_np_s_ds(text):\n",
    "    #delete punct, lemmetize, delete stops\n",
    "    txt = non_letters.sub(\" \", text) \n",
    "    # Convert to lower case, split into individual words\n",
    "    words = txt.lower().split()\n",
    "    return  [st.stem(w) for w in words if w not in stops]\n",
    "\n",
    "def tokenize_np_s(text):\n",
    "    #delete punct, lemmetize, delete stops\n",
    "    txt = non_letters.sub(\" \", text) \n",
    "    # Convert to lower case, split into individual words\n",
    "    words = txt.lower().split()\n",
    "    return  [st.stem(w) for w in words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For CountVectoriser and for <function tokenize_np_nl_ds at 0x120001158>\n",
      "accuracy_score is:    0.978886756238\n",
      "------------->  tfidf  <------------\n",
      "accuracy_score is:    0.980806142035\n",
      "\n",
      "For CountVectoriser and for <function tokenize_np_nl at 0x120001730>\n",
      "accuracy_score is:    0.982085732566\n",
      "------------->  tfidf  <------------\n",
      "accuracy_score is:    0.984644913628\n",
      "\n",
      "For CountVectoriser and for <function tokenize_p_nl_ds at 0x120001400>\n",
      "accuracy_score is:    0.982725527831\n",
      "------------->  tfidf  <------------\n",
      "accuracy_score is:    0.985284708893\n",
      "\n",
      "For CountVectoriser and for <function tokenize_p_nl at 0x1200017b8>\n",
      "accuracy_score is:    0.98720409469\n",
      "------------->  tfidf  <------------\n",
      "accuracy_score is:    0.985284708893\n",
      "\n",
      "For CountVectoriser and for <function tokenize_p_l_ds at 0x120001d90>\n",
      "accuracy_score is:    0.983365323097\n",
      "------------->  tfidf  <------------\n",
      "accuracy_score is:    0.985284708893\n",
      "\n",
      "For CountVectoriser and for <function tokenize_p_l at 0x120001b70>\n",
      "accuracy_score is:    0.987843889955\n",
      "------------->  tfidf  <------------\n",
      "accuracy_score is:    0.985924504159\n",
      "\n",
      "For CountVectoriser and for <function tokenize_np_l_ds at 0x1200011e0>\n",
      "accuracy_score is:    0.980166346769\n",
      "------------->  tfidf  <------------\n",
      "accuracy_score is:    0.982725527831\n",
      "\n",
      "For CountVectoriser and for <function tokenize_np_l at 0x1200019d8>\n",
      "accuracy_score is:    0.980806142035\n",
      "------------->  tfidf  <------------\n",
      "accuracy_score is:    0.982085732566\n",
      "\n",
      "For CountVectoriser and for <function tokenize_p_s_ds at 0x120001ae8>\n",
      "accuracy_score is:    0.982725527831\n",
      "------------->  tfidf  <------------\n",
      "accuracy_score is:    0.983365323097\n",
      "\n",
      "For CountVectoriser and for <function tokenize_p_s at 0x120001a60>\n",
      "accuracy_score is:    0.986564299424\n",
      "------------->  tfidf  <------------\n",
      "accuracy_score is:    0.983365323097\n",
      "\n",
      "For CountVectoriser and for <function tokenize_np_s_ds at 0x120001ea0>\n",
      "accuracy_score is:    0.974408189379\n",
      "------------->  tfidf  <------------\n",
      "accuracy_score is:    0.976327575176\n",
      "\n",
      "For CountVectoriser and for <function tokenize_np_s at 0x1200010d0>\n",
      "accuracy_score is:    0.980166346769\n",
      "------------->  tfidf  <------------\n",
      "accuracy_score is:    0.978246960972\n",
      "The best combo:\n",
      "<class 'list'>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "parametrs = [tokenize_np_nl_ds,\n",
    "            tokenize_np_nl,\n",
    "            tokenize_p_nl_ds,\n",
    "            tokenize_p_nl,\n",
    "            tokenize_p_l_ds,\n",
    "            tokenize_p_l,\n",
    "            tokenize_np_l_ds,\n",
    "            tokenize_np_l,\n",
    "            tokenize_p_s_ds,\n",
    "            tokenize_p_s,\n",
    "            tokenize_np_s_ds,\n",
    "            tokenize_np_s,\n",
    "            ]\n",
    "\n",
    "#cv = CountVectorizer()\n",
    "#gs = (cv, [{'analyzer':'word','tokenizer':parametrs}])\n",
    "\n",
    "clf = MultinomialNB()\n",
    "all_params = {}\n",
    "for param in parametrs:\n",
    "    cv = CountVectorizer(analyzer='word', tokenizer=param)\n",
    "    cv.fit_transform(balansed_messages['message'])\n",
    "    transformed_train = cv.transform(train['message'])\n",
    "    transformed_test = cv.transform(test['message'])\n",
    "    clf.fit(transformed_train, train['label'])\n",
    "    print()\n",
    "    print('For CountVectoriser and for '+ str(param))\n",
    "    print('accuracy_score is:    '+ str(accuracy_score(test['label'], clf.predict(transformed_test))))\n",
    "    all_params[accuracy_score( clf.predict(transformed_test), test['label'])] = str(param) + '  +  count'\n",
    "    print('------------->  tfidf  <------------')\n",
    "    \n",
    "    tcv = TfidfVectorizer(analyzer='word', tokenizer=param)\n",
    "    tcv.fit_transform(balansed_messages['message'])\n",
    "    transformed_train = tcv.transform(train['message'])\n",
    "    transformed_test = tcv.transform(test['message'])\n",
    "    clf.fit(transformed_train, train['label'])\n",
    "    print('accuracy_score is:    '+ str(accuracy_score( test['label'], clf.predict(transformed_test))))\n",
    "    all_params[accuracy_score( clf.predict(transformed_test), test['label'])] = str(param) + '  +  tfidf'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best combo:\n",
      "<function tokenize_p_l at 0x120001b70>  +  count\n"
     ]
    }
   ],
   "source": [
    "print('The best combo:')\n",
    "print(all_params[sorted(all_params)[len(all_params)-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "как мы видим выше, лучшая обработка - лемматизация без удаления стоп слов и пунктуации. Что, наверное, логично, так как в спам-сообщениях, может быть измененная пунктуация и много стоп слов.\n",
    "\n",
    "Теперь на лучшей обработке построим бейзлайн"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "       spam       0.62      0.61      0.62       973\n",
      "        ham       0.38      0.39      0.39       590\n",
      "\n",
      "avg / total       0.53      0.53      0.53      1563\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "cv = CountVectorizer(analyzer='word', tokenizer=tokenize_p_l)\n",
    "cv.fit_transform(balansed_messages['message'])\n",
    "transformed_train = cv.transform(train['message'])\n",
    "transformed_test = cv.transform(test['message'])\n",
    "\n",
    "\n",
    "dc = DummyClassifier()\n",
    "dc.fit(transformed_train, train['label'])\n",
    "\n",
    "print(classification_report(test['label'], dc.predict(transformed_test), target_names=['spam', 'ham']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       1.00      0.98      0.99       973\n",
      "       spam       0.97      1.00      0.99       590\n",
      "\n",
      "avg / total       0.99      0.99      0.99      1563\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       1.00      1.00      1.00       973\n",
      "       spam       1.00      1.00      1.00       590\n",
      "\n",
      "avg / total       1.00      1.00      1.00      1563\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#train DecisionTreeClassifier\n",
    "dtc = DecisionTreeClassifier()\n",
    "dtc.fit(transformed_train, train['label'])\n",
    "print(classification_report(test['label'], dtc.predict(transformed_test)))\n",
    "\n",
    "#train RandomForestClassifier\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(transformed_train, train['label'])\n",
    "print(classification_report(test['label'], rfc.predict(transformed_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFgUMERGJ5f8D12CXvnPRQB4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11adfd0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import learning_curve, ShuffleSplit\n",
    "\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "estimator = MultinomialNB()\n",
    "plot_learning_curve(estimator, 'Naive Bayes', transformed_train.toarray(),\n",
    "                    train['label'], ylim=(0.7, 1.01), cv=cv, n_jobs=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-d3953eb41a31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0my_probas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformed_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mskplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_roc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_probas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Ksenia/anaconda/lib/python3.6/site-packages/scikitplot/plotters.py\u001b[0m in \u001b[0;36mplot_roc_curve\u001b[0;34m(y_true, y_probas, title, curves, ax, figsize, title_fontsize, text_fontsize)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0mroc_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mfpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0mroc_auc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "import scikitplot.plotters as skplt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(transformed_train, train['label'])\n",
    "y_true = test['label']\n",
    "y_probas = clf.predict(transformed_train)\n",
    "skplt.plot_roc_curve(y_true, y_probas)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
