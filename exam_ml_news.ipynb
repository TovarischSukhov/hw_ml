{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>URL</th>\n",
       "      <th>PUBLISHER</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>STORY</th>\n",
       "      <th>HOSTNAME</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fed official says weak data caused by weather,...</td>\n",
       "      <td>http://www.latimes.com/business/money/la-fi-mo...</td>\n",
       "      <td>Los Angeles Times</td>\n",
       "      <td>b</td>\n",
       "      <td>ddUyU0VZz0BRneMioxUPQVP6sIxvM</td>\n",
       "      <td>www.latimes.com</td>\n",
       "      <td>1394470370698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fed's Charles Plosser sees high bar for change...</td>\n",
       "      <td>http://www.livemint.com/Politics/H2EvwJSK2VE6O...</td>\n",
       "      <td>Livemint</td>\n",
       "      <td>b</td>\n",
       "      <td>ddUyU0VZz0BRneMioxUPQVP6sIxvM</td>\n",
       "      <td>www.livemint.com</td>\n",
       "      <td>1394470371207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US open: Stocks fall after Fed official hints ...</td>\n",
       "      <td>http://www.ifamagazine.com/news/us-open-stocks...</td>\n",
       "      <td>IFA Magazine</td>\n",
       "      <td>b</td>\n",
       "      <td>ddUyU0VZz0BRneMioxUPQVP6sIxvM</td>\n",
       "      <td>www.ifamagazine.com</td>\n",
       "      <td>1394470371550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fed risks falling 'behind the curve', Charles ...</td>\n",
       "      <td>http://www.ifamagazine.com/news/fed-risks-fall...</td>\n",
       "      <td>IFA Magazine</td>\n",
       "      <td>b</td>\n",
       "      <td>ddUyU0VZz0BRneMioxUPQVP6sIxvM</td>\n",
       "      <td>www.ifamagazine.com</td>\n",
       "      <td>1394470371793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Fed's Plosser: Nasty Weather Has Curbed Job Gr...</td>\n",
       "      <td>http://www.moneynews.com/Economy/federal-reser...</td>\n",
       "      <td>Moneynews</td>\n",
       "      <td>b</td>\n",
       "      <td>ddUyU0VZz0BRneMioxUPQVP6sIxvM</td>\n",
       "      <td>www.moneynews.com</td>\n",
       "      <td>1394470372027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                TITLE  \\\n",
       "ID                                                      \n",
       "1   Fed official says weak data caused by weather,...   \n",
       "2   Fed's Charles Plosser sees high bar for change...   \n",
       "3   US open: Stocks fall after Fed official hints ...   \n",
       "4   Fed risks falling 'behind the curve', Charles ...   \n",
       "5   Fed's Plosser: Nasty Weather Has Curbed Job Gr...   \n",
       "\n",
       "                                                  URL          PUBLISHER  \\\n",
       "ID                                                                         \n",
       "1   http://www.latimes.com/business/money/la-fi-mo...  Los Angeles Times   \n",
       "2   http://www.livemint.com/Politics/H2EvwJSK2VE6O...           Livemint   \n",
       "3   http://www.ifamagazine.com/news/us-open-stocks...       IFA Magazine   \n",
       "4   http://www.ifamagazine.com/news/fed-risks-fall...       IFA Magazine   \n",
       "5   http://www.moneynews.com/Economy/federal-reser...          Moneynews   \n",
       "\n",
       "   CATEGORY                          STORY             HOSTNAME      TIMESTAMP  \n",
       "ID                                                                              \n",
       "1         b  ddUyU0VZz0BRneMioxUPQVP6sIxvM      www.latimes.com  1394470370698  \n",
       "2         b  ddUyU0VZz0BRneMioxUPQVP6sIxvM     www.livemint.com  1394470371207  \n",
       "3         b  ddUyU0VZz0BRneMioxUPQVP6sIxvM  www.ifamagazine.com  1394470371550  \n",
       "4         b  ddUyU0VZz0BRneMioxUPQVP6sIxvM  www.ifamagazine.com  1394470371793  \n",
       "5         b  ddUyU0VZz0BRneMioxUPQVP6sIxvM    www.moneynews.com  1394470372027  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame.from_csv('./uci-news-aggregator.csv',\n",
    "                           #sep=',',\n",
    "                           #names='ID TITLE URL PUBLISHER CATEGORY STORY HOSTNAME TIMESTAMP'.split(' '),\n",
    "                           #low_memory=False,\n",
    "                           #indexer='ID'\n",
    "                  )\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "по идее нам нужны только названия и категории. Сформируем нужный датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CATEGORY</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">b</th>\n",
       "      <th>count</th>\n",
       "      <td>115967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>111903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Posted by Imaduddin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">e</th>\n",
       "      <th>count</th>\n",
       "      <td>152469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>146952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>The article requested cannot be found! Please ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">m</th>\n",
       "      <th>count</th>\n",
       "      <td>45639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>43719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Share this on:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">t</th>\n",
       "      <th>count</th>\n",
       "      <td>108344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>104733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Business Wire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             TITLE\n",
       "CATEGORY                                                          \n",
       "b        count                                              115967\n",
       "         unique                                             111903\n",
       "         top                                   Posted by Imaduddin\n",
       "         freq                                                   52\n",
       "e        count                                              152469\n",
       "         unique                                             146952\n",
       "         top     The article requested cannot be found! Please ...\n",
       "         freq                                                  130\n",
       "m        count                                               45639\n",
       "         unique                                              43719\n",
       "         top                                        Share this on:\n",
       "         freq                                                    9\n",
       "t        count                                              108344\n",
       "         unique                                             104733\n",
       "         top                                         Business Wire\n",
       "         freq                                                   29"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_data = data[['TITLE','CATEGORY']]\n",
    "classification_data.groupby('CATEGORY').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "видно, что всего 4 класса и они, в принципе, не очень разбаллансированы. Выберем лучшую векторизацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(classification_data, test_size=0.2, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import re\n",
    "\n",
    "st = LancasterStemmer()\n",
    "\n",
    "lmtzr = WordNetLemmatizer()\n",
    "tknzr = TweetTokenizer()\n",
    "punct = re.compile(',|\\.|;|:|\\?|\\)|\\(|!')\n",
    "non_letters = re.compile('[^a-zA-Z]')\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "def tokenize_np_nl_ds(text):\n",
    "    #no punctuation, no lemmatisation, delete stops\n",
    "    letters_only = non_letters.sub(\" \", text) \n",
    "    words = letters_only.lower().split()                 \n",
    "    meaningful_words = [w for w in words if w not in stops]   \n",
    "    return meaningful_words \n",
    "\n",
    "def tokenize_np_nl(text):\n",
    "    #delete punctuation, no lemmatisation, leave stops\n",
    "    letters_only = non_letters.sub(\" \", text) \n",
    "    words = letters_only.lower().split()                   \n",
    "    return  words \n",
    "\n",
    "def tokenize_p_nl_ds(text):\n",
    "    #leave punct, no lemmatisation, delete stops\n",
    "    txt = tknzr.tokenize(text)\n",
    "    meaningful_words = [w for w in txt if w not in stops]   \n",
    "    return meaningful_words \n",
    "\n",
    "def tokenize_p_nl(text):\n",
    "    #leave punct, no lemmatisation, leave stops\n",
    "    txt = tknzr.tokenize(text)\n",
    "    return  txt\n",
    "\n",
    "def tokenize_p_l_ds(text):\n",
    "    #leave punct, lemmetize, delete stops\n",
    "    txt = tknzr.tokenize(text)\n",
    "    meaningful_words = [lmtzr.lemmatize(w) for w in txt if w not in stops]   \n",
    "    return  meaningful_words \n",
    "\n",
    "def tokenize_p_l(text):\n",
    "    #leave punct, lemmetize, leave stops\n",
    "    txt = tknzr.tokenize(text) \n",
    "    return [lmtzr.lemmatize(w) for w in txt]\n",
    "\n",
    "def tokenize_np_l_ds(text):\n",
    "    #delete punct, lemmetize, delete stops\n",
    "    txt = non_letters.sub(\" \", text) \n",
    "    # Convert to lower case, split into individual words\n",
    "    words = txt.lower().split()\n",
    "    return  [lmtzr.lemmatize(w) for w in words if w not in stops]\n",
    "\n",
    "def tokenize_np_l(text):\n",
    "    #delete punct, lemmetize, delete stops\n",
    "    txt = non_letters.sub(\" \", text) \n",
    "    # Convert to lower case, split into individual words\n",
    "    words = txt.lower().split()\n",
    "    return  [lmtzr.lemmatize(w) for w in words]\n",
    "\n",
    "\n",
    "\n",
    "#same for stemmatisation\n",
    "def tokenize_p_s_ds(text):\n",
    "    #leave punct, lemmetize, delete stops\n",
    "    txt = tknzr.tokenize(text)\n",
    "    meaningful_words = [st.stem(w) for w in txt if w not in stops]   \n",
    "    return  meaningful_words \n",
    "\n",
    "def tokenize_p_s(text):\n",
    "    #leave punct, lemmetize, leave stops\n",
    "    txt = tknzr.tokenize(text) \n",
    "    return [st.stem(w) for w in txt]\n",
    "\n",
    "def tokenize_np_s_ds(text):\n",
    "    #delete punct, lemmetize, delete stops\n",
    "    txt = non_letters.sub(\" \", text) \n",
    "    # Convert to lower case, split into individual words\n",
    "    words = txt.lower().split()\n",
    "    return  [st.stem(w) for w in words if w not in stops]\n",
    "\n",
    "def tokenize_np_s(text):\n",
    "    #delete punct, lemmetize, delete stops\n",
    "    txt = non_letters.sub(\" \", text) \n",
    "    # Convert to lower case, split into individual words\n",
    "    words = txt.lower().split()\n",
    "    return  [st.stem(w) for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For CountVectoriser and for <function tokenize_np_nl_ds at 0x112fb7bf8>\n",
      "accuracy_score is:    0.927110458785\n",
      "------------->  tfidf  <------------\n",
      "accuracy_score is:    0.92548884996\n",
      "\n",
      "For CountVectoriser and for <function tokenize_np_nl at 0x112fb79d8>\n",
      "accuracy_score is:    0.924672127267\n",
      "------------->  tfidf  <------------\n",
      "accuracy_score is:    0.922908479712\n",
      "\n",
      "For CountVectoriser and for <function tokenize_p_nl_ds at 0x112fb7c80>\n",
      "accuracy_score is:    0.92688556413\n",
      "------------->  tfidf  <------------\n",
      "accuracy_score is:    0.925169262819\n",
      "\n",
      "For CountVectoriser and for <function tokenize_p_nl at 0x112fb7b70>\n",
      "accuracy_score is:    0.924423559491\n",
      "------------->  tfidf  <------------\n",
      "accuracy_score is:    0.922979499077\n",
      "\n",
      "For CountVectoriser and for <function tokenize_p_l_ds at 0x112fb7d90>\n",
      "accuracy_score is:    0.924766819753\n",
      "------------->  tfidf  <------------\n",
      "accuracy_score is:    0.923062355002\n",
      "\n",
      "For CountVectoriser and for <function tokenize_p_l at 0x112fb7e18>\n",
      "accuracy_score is:    0.922517873207\n",
      "------------->  tfidf  <------------\n",
      "accuracy_score is:    0.921061976232\n",
      "\n",
      "For CountVectoriser and for <function tokenize_np_l_ds at 0x112fb7ea0>\n",
      "accuracy_score is:    0.925370484352\n",
      "------------->  tfidf  <------------\n",
      "accuracy_score is:    0.923630509919\n",
      "\n",
      "For CountVectoriser and for <function tokenize_np_l at 0x112fb7f28>\n",
      "accuracy_score is:    0.922908479712\n",
      "------------->  tfidf  <------------\n",
      "accuracy_score is:    0.921653804271\n",
      "\n",
      "For CountVectoriser and for <function tokenize_p_s_ds at 0x112fb7a60>\n",
      "accuracy_score is:    0.917309786468\n",
      "------------->  tfidf  <------------\n",
      "accuracy_score is:    0.915344917381\n",
      "\n",
      "For CountVectoriser and for <function tokenize_p_s at 0x112e1f1e0>\n",
      "accuracy_score is:    0.915415936745\n",
      "------------->  tfidf  <------------\n",
      "accuracy_score is:    0.913746981677\n",
      "\n",
      "For CountVectoriser and for <function tokenize_np_s_ds at 0x112fe2048>\n",
      "accuracy_score is:    0.916019601345\n",
      "------------->  tfidf  <------------\n",
      "accuracy_score is:    0.915202878652\n",
      "\n",
      "For CountVectoriser and for <function tokenize_np_s at 0x112fe20d0>\n",
      "accuracy_score is:    0.913877183845\n",
      "------------->  tfidf  <------------\n",
      "accuracy_score is:    0.913368211732\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "parametrs = [tokenize_np_nl_ds,\n",
    "            tokenize_np_nl,\n",
    "            tokenize_p_nl_ds,\n",
    "            tokenize_p_nl,\n",
    "            tokenize_p_l_ds,\n",
    "            tokenize_p_l,\n",
    "            tokenize_np_l_ds,\n",
    "            tokenize_np_l,\n",
    "            tokenize_p_s_ds,\n",
    "            tokenize_p_s,\n",
    "            tokenize_np_s_ds,\n",
    "            tokenize_np_s,\n",
    "            ]\n",
    "\n",
    "clf = MultinomialNB()\n",
    "all_params = {}\n",
    "for param in parametrs:\n",
    "    cv = CountVectorizer(analyzer='word', tokenizer=param)\n",
    "    cv.fit_transform(classification_data['TITLE'])\n",
    "    transformed_train = cv.transform(train['TITLE'])\n",
    "    transformed_test = cv.transform(test['TITLE'])\n",
    "    clf.fit(transformed_train, train['CATEGORY'])\n",
    "    print()\n",
    "    print('For CountVectoriser and for '+ str(param))\n",
    "    print('accuracy_score is:    '+ str(accuracy_score(test['CATEGORY'], clf.predict(transformed_test))))\n",
    "    all_params[accuracy_score( clf.predict(transformed_test), test['CATEGORY'])] = str(param) + '  +  count'\n",
    "    print('------------->  tfidf  <------------')\n",
    "    \n",
    "    tcv = TfidfVectorizer(analyzer='word', tokenizer=param)\n",
    "    tcv.fit_transform(classification_data['TITLE'])\n",
    "    transformed_train = tcv.transform(train['TITLE'])\n",
    "    transformed_test = tcv.transform(test['TITLE'])\n",
    "    clf.fit(transformed_train, train['CATEGORY'])\n",
    "    print('accuracy_score is:    '+ str(accuracy_score( test['CATEGORY'], clf.predict(transformed_test))))\n",
    "    all_params[accuracy_score( clf.predict(transformed_test), test['CATEGORY'])] = str(param) + '  +  tfidf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best combo:\n",
      "<function tokenize_np_nl_ds at 0x112fb7bf8>  +  count\n"
     ]
    }
   ],
   "source": [
    "print('The best combo:')\n",
    "print(all_params[sorted(all_params)[len(all_params)-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "построим бейзлайн.\n",
    "Для этого еще раз разделим на train и test, по которым мы будем проверять модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "to_train, to_test = train_test_split(train, test_size=0.3, random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          b       0.28      0.28      0.28     27882\n",
      "          e       0.36      0.36      0.36     36525\n",
      "          m       0.11      0.11      0.11     10943\n",
      "          t       0.26      0.26      0.26     26031\n",
      "\n",
      "avg / total       0.29      0.29      0.29    101381\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "cv = CountVectorizer(analyzer='word', tokenizer=tokenize_np_nl_ds)\n",
    "cv.fit_transform(classification_data['TITLE'])\n",
    "transformed_to_train = cv.transform(to_train['TITLE'])\n",
    "transformed_to_test = cv.transform(to_test['TITLE'])\n",
    "transformed_test = cv.transform(test['TITLE'])\n",
    "\n",
    "dc = DummyClassifier()\n",
    "dc.fit(transformed_to_train, to_train['CATEGORY'])\n",
    "#print(to_test['Line'].tolist())\n",
    "print(classification_report(to_test['CATEGORY'], dc.predict(transformed_to_test), target_names=['b', 'e', 'm' , 't']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = [{'penalty':['l1','l2'], 'C':[0.1, 0.5, 1.0, 2, 5]}]\n",
    "\n",
    "lr = LogisticRegression()#solver='sag', multi_class='multinomial', n_jobs=-1, max_iter=100)\n",
    "\n",
    "clf = GridSearchCV(lr, params)\n",
    "clf.fit(transformed_to_train, to_train['CATEGORY']) \n",
    "print(clf.best_params_, clf.best_score_)\n",
    "\n",
    "print(classification_report(to_test['CATEGORY'], clf.predict(transformed_to_test), target_names=['b', 'e', 'm' , 't']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "mat = confusion_matrix(to_test['CATEGORY'], clf.predict(transformed_to_test))\n",
    "plt.figure()\n",
    "plot_confusion_matrix(mat, classes=['b', 'e', 'm' , 't'],\n",
    "                      title='Confusion matrix. Logistic Regression')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
