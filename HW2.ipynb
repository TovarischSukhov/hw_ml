{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Домашнее задание #2.\n",
    "\n",
    "SMS Spam/Ham dataset.\n",
    "1(+6). Проверить, сбалансирован ли датасет (может быть, наблюдений одного класса слишком много?). Какие результаты покажет dummy classifier, который будет всем новым наблюдениям присваивать класс ham? Насколько плохо такое решение для задачи определения спама?\n",
    "Грубое решение - включить в training set только необходимое число наблюдений (примерно поровну spam и ham). \n",
    "Нормализовать тексты и обучить байесовскую модель (bag of words). Проверить, как влияют на результат:\n",
    "1) разная токенизация: в одном случае знаки препинания удалять, в другом — считать их токенами;\n",
    "2) лемматизация (отсутствие лемматизации, стемминг, лемматизация; инструменты можно использовать любые, например, nltk.stem);\n",
    "3) удаление стоп-слов, а также пороги минимальной и максимальной document frequency;\n",
    "4) векторизация документов (CountVectorizer vs. TfIdfVectorizer);\n",
    "5) что-нибудь ещё?\n",
    "При оценке классификатора обратите внимание на TP и FP.\n",
    "\n",
    "Extra: ограничив количество наблюдений ham в обучающей выборке, мы игнорируем довольно много данных. 1) В цикле: случайно выбрать нужное число писем ham и сконструировать сбалансированную выборку, построить классификатор, оценить и записать результат; в итоге результаты усреднить. 2) поможет ли параметр class prior probability?\n",
    "\n",
    "\n",
    "2(+2). Сравнить результаты байесовского классификатора, решающего дерева и RandomForest. Помимо стандартных метрик оценки качества модели, необходимо построить learning curve, ROC-curve, classification report и интерпретировать эти результаты.\n",
    "\n",
    "3(+2). А что, если в качестве предикторов брать не количество вхождений слов, а конструировать специальные признаки? Прежде всего, необходимо разделить таблицу на training set и test set в соотношении 80:20, test set не открывать до этапа оценки модели. С помощью pandas проверить, отличаются ли перечисленные ниже параметры (иможно придумать другие) для разных классов (spam/ham), и собрать матрицу признаков для обучения. Примеры признаков: длина сообщения, количество букв в ВЕРХНЕМ РЕГИСТРЕ, восклицательных знаков, цифр, запятых, каких-то конкретных слов (для этого можно построить частотный словарь по сообщениям каждого класса). Прокомментировать свой выбор. Векторизовать документы и построить классификатор. Оценить модель на проверочной выборке.\n",
    "\n",
    "\n",
    "http://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                        message\n",
      "label                                                          \n",
      "ham   count                                                4825\n",
      "      unique                                               4516\n",
      "      top                                Sorry, I'll call later\n",
      "      freq                                                   30\n",
      "spam  count                                                 747\n",
      "      unique                                                653\n",
      "      top     Please call our customer service representativ...\n",
      "      freq                                                    4\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "\n",
    "messages = pandas.read_csv('./smsspamcollection/SMSSpamCollection',\n",
    "                           sep='\\t',\n",
    "                           names=[\"label\", \"message\"])\n",
    "print(messages.groupby('label').describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                        message\n",
      "label                                                          \n",
      "ham   count                                                4825\n",
      "      unique                                               4516\n",
      "      top                                Sorry, I'll call later\n",
      "      freq                                                   30\n",
      "spam  count                                                2988\n",
      "      unique                                                653\n",
      "      top     Please call our customer service representativ...\n",
      "      freq                                                   16\n"
     ]
    }
   ],
   "source": [
    "#balansing the data\n",
    "balansed_messages_ = messages.append(messages[messages.label == 'spam'])\n",
    "balansed_messages = balansed_messages_.append(balansed_messages_[balansed_messages_.label == 'spam'])\n",
    "print(balansed_messages.groupby('label').describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#X, y = balansed_messages['message'], balansed_messages['label']\n",
    "train, test = train_test_split(balansed_messages,\n",
    "         test_size=0.2, random_state=42)\n",
    "#print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import re\n",
    "\n",
    "st = LancasterStemmer()\n",
    "\n",
    "lmtzr = WordNetLemmatizer()\n",
    "tknzr = TweetTokenizer()\n",
    "punct = re.compile(',|\\.|;|:|\\?|\\)|\\(|!')\n",
    "non_letters = re.compile('[^a-zA-Z]')\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "def tokenize_np_nl_ds(text):\n",
    "    #no punctuation, no lemmatisation, delete stops\n",
    "    letters_only = non_letters.sub(\" \", text) \n",
    "    # Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                 \n",
    "    # Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    # Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return ( \" \".join( meaningful_words ))\n",
    "\n",
    "def tokenize_np_nl(text):\n",
    "    #no punctuation, no lemmatisation, leave stops\n",
    "    letters_only = non_letters.sub(\" \", text) \n",
    "    # Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                   \n",
    "    # Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return ( \" \".join( words ))\n",
    "\n",
    "def tokenize_p_nl_ds(text):\n",
    "    #leave punct, no lemmatisation, delete stops\n",
    "    txt = tknzr.tokenize(text)\n",
    "    # Remove stop words\n",
    "    meaningful_words = [w for w in txt if not w in stops]   \n",
    "    # Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return ( \" \".join( meaningful_words ))\n",
    "\n",
    "def tokenize_p_nl(text):\n",
    "    #leave punct, no lemmatisation, leave stops\n",
    "    txt = tknzr.tokenize(text)\n",
    "    # Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return ( \" \".join( txt ))\n",
    "\n",
    "def tokenize_p_l_ds(text):\n",
    "    #leave punct, lemmetize, delete stops\n",
    "    txt = tknzr.tokenize(text)\n",
    "    txt = lmtzr.lemmatize(txt)\n",
    "    meaningful_words = [w for w in txt if not w in stops]   \n",
    "    return ( \" \".join( meaningful_words ))\n",
    "\n",
    "def tokenize_p_l(text):\n",
    "    #leave punct, lemmetize, leave stops\n",
    "    txt = tknzr.tokenize(text)\n",
    "    txt = lmtzr.lemmatize(txt)  \n",
    "    return ( \" \".join( txt ))\n",
    "\n",
    "def tokenize_np_l_ds(text):\n",
    "    #delete punct, lemmetize, delete stops\n",
    "    txt = non_letters.sub(\" \", text) \n",
    "    # Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()\n",
    "    # Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    txt = lmtzr.lemmatize(meaningful_words)\n",
    "    return ( \" \".join( txt ))\n",
    "\n",
    "def tokenize_np_s(text):\n",
    "    #delete punct, lemmetize, leave stops\n",
    "    txt = non_letters.sub(\" \", text) \n",
    "    # Convert to lower case, split into individual words\n",
    "    words = txt.lower().split() \n",
    "    txt = st.stem(words)  \n",
    "    return ( \" \".join( txt ))\n",
    "\n",
    "#same for stemmatisation\n",
    "def tokenize_p_s_ds(text):\n",
    "    #leave punct, lemmetize, delete stops\n",
    "    txt = tknzr.tokenize(text)\n",
    "    meaningful_words = [w for w in txt if not w in stops] \n",
    "    txt = st.stem(meaningful_words)\n",
    "    return ( \" \".join( txt ))\n",
    "\n",
    "def tokenize_s_l(text):\n",
    "    #leave punct, lemmetize, leave stops\n",
    "    txt = tknzr.tokenize(text)\n",
    "    txt = st.stem(txt)  \n",
    "    return ( \" \".join( txt ))\n",
    "\n",
    "def tokenize_np_s_ds(text):\n",
    "    #delete punct, lemmetize, delete stops\n",
    "    txt = non_letters.sub(\" \", text) \n",
    "    # Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()\n",
    "    # Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    txt = st.stem(meaningful_words)\n",
    "    return ( \" \".join( txt ))\n",
    "\n",
    "def tokenize_np_s(text):\n",
    "    #delete punct, lemmetize, leave stops\n",
    "    txt = non_letters.sub(\" \", text) \n",
    "    # Convert to lower case, split into individual words\n",
    "    words = txt.lower().split() \n",
    "    txt = st.stem(words)  \n",
    "    return ( \" \".join( txt ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For CountVectoriser and for <function tokenize_np_nl_ds at 0x114dbef28>\n",
      "classification report is:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.87      0.88      0.87       962\n",
      "       spam       0.80      0.78      0.79       601\n",
      "\n",
      "avg / total       0.84      0.84      0.84      1563\n",
      "\n",
      "\n",
      "f1_score is:\n",
      "\n",
      "accuracy_score is:\n",
      "0.840690978887\n",
      "\n",
      "confusion_matrix is:\n",
      "[[843 119]\n",
      " [130 471]]\n",
      "\n",
      "For TfIdfVectoriser and for <function tokenize_np_nl_ds at 0x114dbef28>\n",
      "classification report is:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.99      0.63      0.77      1525\n",
      "       spam       0.05      0.84      0.10        38\n",
      "\n",
      "avg / total       0.97      0.64      0.76      1563\n",
      "\n",
      "\n",
      "f1_score is:\n",
      "\n",
      "accuracy_score is:\n",
      "0.63915547025\n",
      "\n",
      "confusion_matrix is:\n",
      "[[967 558]\n",
      " [  6  32]]\n",
      "\n",
      "For CountVectoriser and for <function tokenize_np_nl at 0x11a1cfea0>\n",
      "classification report is:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.87      0.89      0.88       953\n",
      "       spam       0.82      0.79      0.81       610\n",
      "\n",
      "avg / total       0.85      0.85      0.85      1563\n",
      "\n",
      "\n",
      "f1_score is:\n",
      "\n",
      "accuracy_score is:\n",
      "0.851567498401\n",
      "\n",
      "confusion_matrix is:\n",
      "[[847 106]\n",
      " [126 484]]\n",
      "\n",
      "For TfIdfVectoriser and for <function tokenize_np_nl at 0x11a1cfea0>\n",
      "classification report is:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       1.00      0.63      0.77      1541\n",
      "       spam       0.03      0.82      0.06        22\n",
      "\n",
      "avg / total       0.98      0.63      0.76      1563\n",
      "\n",
      "\n",
      "f1_score is:\n",
      "\n",
      "accuracy_score is:\n",
      "0.631477927063\n",
      "\n",
      "confusion_matrix is:\n",
      "[[969 572]\n",
      " [  4  18]]\n",
      "\n",
      "For CountVectoriser and for <function tokenize_p_nl_ds at 0x11a1cfe18>\n",
      "classification report is:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.98      0.94      0.96      1019\n",
      "       spam       0.89      0.97      0.93       544\n",
      "\n",
      "avg / total       0.95      0.95      0.95      1563\n",
      "\n",
      "\n",
      "f1_score is:\n",
      "\n",
      "accuracy_score is:\n",
      "0.947536788228\n",
      "\n",
      "confusion_matrix is:\n",
      "[[955  64]\n",
      " [ 18 526]]\n",
      "\n",
      "For TfIdfVectoriser and for <function tokenize_p_nl_ds at 0x11a1cfe18>\n",
      "classification report is:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.99      0.92      0.95      1037\n",
      "       spam       0.87      0.97      0.92       526\n",
      "\n",
      "avg / total       0.95      0.94      0.94      1563\n",
      "\n",
      "\n",
      "f1_score is:\n",
      "\n",
      "accuracy_score is:\n",
      "0.941138835573\n",
      "\n",
      "confusion_matrix is:\n",
      "[[959  78]\n",
      " [ 14 512]]\n",
      "\n",
      "For CountVectoriser and for <function tokenize_p_nl at 0x11a1cfd90>\n",
      "classification report is:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.98      0.94      0.96      1016\n",
      "       spam       0.89      0.96      0.92       547\n",
      "\n",
      "avg / total       0.95      0.94      0.94      1563\n",
      "\n",
      "\n",
      "f1_score is:\n",
      "\n",
      "accuracy_score is:\n",
      "0.9443378119\n",
      "\n",
      "confusion_matrix is:\n",
      "[[951  65]\n",
      " [ 22 525]]\n",
      "\n",
      "For TfIdfVectoriser and for <function tokenize_p_nl at 0x11a1cfd90>\n",
      "classification report is:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.99      0.92      0.95      1051\n",
      "       spam       0.85      0.98      0.91       512\n",
      "\n",
      "avg / total       0.94      0.94      0.94      1563\n",
      "\n",
      "\n",
      "f1_score is:\n",
      "\n",
      "accuracy_score is:\n",
      "0.93730006398\n",
      "\n",
      "confusion_matrix is:\n",
      "[[963  88]\n",
      " [ 10 502]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "parametrs = [tokenize_np_nl_ds,\n",
    "              tokenize_np_nl,\n",
    "              tokenize_p_nl_ds,\n",
    "              tokenize_p_nl,\n",
    "              #tokenize_p_l_ds,\n",
    "              #tokenize_p_l,\n",
    "              #tokenize_np_l_ds,\n",
    "              #tokenize_np_s,\n",
    "              #tokenize_p_s_ds,\n",
    "              #tokenize_s_l,\n",
    "              #tokenize_np_s_ds,\n",
    "              #tokenize_np_s\n",
    "            ]\n",
    "\n",
    "clf = MultinomialNB()\n",
    "\n",
    "for param in parametrs:\n",
    "    cv = CountVectorizer(analyzer='word', tokenizer=param)\n",
    "    cv.fit_transform(balansed_messages['message'])\n",
    "    transformed_train = cv.transform(train['message'])\n",
    "    transformed_test = cv.transform(test['message'])\n",
    "    clf.fit(transformed_train, train['label'])\n",
    "    print('For CountVectoriser and for '+ str(param))\n",
    "    print('classification report is:')\n",
    "    print(classification_report( clf.predict(transformed_test), test['label']))\n",
    "    print()\n",
    "    print('f1_score is:')\n",
    "    #print(f1_score( clf.predict(transformed_test), test['label']))\n",
    "    print()\n",
    "    print('accuracy_score is:')\n",
    "    print(accuracy_score( clf.predict(transformed_test), test['label']))\n",
    "    print()\n",
    "    print('confusion_matrix is:')\n",
    "    print(confusion_matrix( clf.predict(transformed_test), test['label']))\n",
    "    print()\n",
    "    \n",
    "    tcv = TfidfVectorizer(analyzer='word', tokenizer=param)\n",
    "    tcv.fit_transform(balansed_messages['message'])\n",
    "    transformed_train = tcv.transform(train['message'])\n",
    "    transformed_test = tcv.transform(test['message'])\n",
    "    clf.fit(transformed_train, train['label'])\n",
    "    print('For TfIdfVectoriser and for '+ str(param))\n",
    "    print('classification report is:')\n",
    "    print(classification_report( clf.predict(transformed_test), test['label']))\n",
    "    print()\n",
    "    print('f1_score is:')\n",
    "    #print(f1_score( clf.predict(transformed_test), test['label']))\n",
    "    print()\n",
    "    print('accuracy_score is:')\n",
    "    print(accuracy_score( clf.predict(transformed_test), test['label']))\n",
    "    print()\n",
    "    print('confusion_matrix is:')\n",
    "    print(confusion_matrix( clf.predict(transformed_test), test['label']))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.98      1.00      0.99       951\n",
      "       spam       1.00      0.96      0.98       612\n",
      "\n",
      "avg / total       0.99      0.99      0.99      1563\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.99      1.00      0.99       966\n",
      "       spam       0.99      0.98      0.99       597\n",
      "\n",
      "avg / total       0.99      0.99      0.99      1563\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#chose best vectosiser\n",
    "cv = CountVectorizer(analyzer='word', tokenizer=tokenize_p_nl_ds)\n",
    "cv.fit_transform(balansed_messages['message'])\n",
    "transformed_train = cv.transform(train['message'])\n",
    "transformed_test = cv.transform(test['message'])\n",
    "\n",
    "#train DecisionTreeClassifier\n",
    "dtc = DecisionTreeClassifier()\n",
    "dtc.fit(transformed_train, train['label'])\n",
    "print(classification_report( dtc.predict(transformed_test), test['label']))\n",
    "\n",
    "#train RandomForestClassifier\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(transformed_train, train['label'])\n",
    "print(classification_report( rfc.predict(transformed_test), test['label']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6250\n",
      "1563\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (5,) and (4166,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-9e0ae6f30c7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m#                 test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m plt.plot(np.linspace(.1, 1.0, 5), train_scores_mean, 'o-', color=\"r\",\n\u001b[0;32m---> 23\u001b[0;31m          label=\"Training score\")\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;31m#plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#         label=\"Cross-validation score\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Ksenia/anaconda/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   3316\u001b[0m                       mplDeprecation)\n\u001b[1;32m   3317\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3318\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3319\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3320\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwashold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Ksenia/anaconda/lib/python3.6/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1890\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[1;32m   1891\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1892\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1893\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1894\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Ksenia/anaconda/lib/python3.6/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1404\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_alias_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1406\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1407\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1408\u001b[0m             \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Ksenia/anaconda/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_grab_next_args\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    405\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mseg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mseg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Ksenia/anaconda/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'plot'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Ksenia/anaconda/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m             raise ValueError(\"x and y must have same first dimension, but \"\n\u001b[0;32m--> 244\u001b[0;31m                              \"have shapes {} and {}\".format(x.shape, y.shape))\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             raise ValueError(\"x and y can be no greater than 2-D, but have \"\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (5,) and (4166,)"
     ]
    }
   ],
   "source": [
    "print(train['message'].size)\n",
    "print(test['message'].size)\n",
    "\n",
    "plt.figure()\n",
    "plt.title('smth')\n",
    "\n",
    "plt.xlabel(\"Training examples\")\n",
    "plt.ylabel(\"Score\")\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "            dtc, transformed_train, train['label'], train_sizes=(range(1, 4167)))\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "plt.grid()\n",
    "\n",
    "#plt.fill_between(train['message'].size, train_scores_mean - train_scores_std,\n",
    "#                 train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "\n",
    "#plt.fill_between(train['message'].size, test_scores_mean - test_scores_std,\n",
    "#                 test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(np.linspace(.1, 1.0, 5), train_scores_mean, 'o-', color=\"r\",\n",
    "         label=\"Training score\")\n",
    "#plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "#         label=\"Cross-validation score\")\n",
    "\n",
    "#plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
