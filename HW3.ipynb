{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построить классификатор, который \"угадывает\" персонажа по его фразе. Baseline: равновероятный выбор между всеми главными героями.\n",
    "Примерный план действий: \n",
    "1. выделить реплики главных персонажей\n",
    "2.сгруппировать их по героям, нормализовать (свой выбор процедур объяснить (например, нужно ли включать в стоп-лист обсценную лексику?); из инструментов для английского удобно использовать nltk)\n",
    "3. векторизовать (например, CountVectorizer), отрезать поверочную выборку (эту часть не трогаем до последней проверки).\n",
    "4. На оставшейся, большей, части данных: \n",
    "1) немного поанализировать данные, чтобы понять, какие признаки могут помочь при обучении — например, построить частотные списки и сравнить их (достаточно ли матрицы терм-документ с ненормализованными вхождениями слов?); \n",
    "2) обучить модели (лес, наивный байес, логит) и подобрать их оптимальные параметры. Интерпретировать параметры (атрибуты) лучших моделей. Протестировать их на проверочной выборке, выбрать лучшую модель, проиллюстрировать результат (например, нарисовать decision surface). Сравнить результат с нехитрым Baseline классификатором."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>Character</th>\n",
       "      <th>Line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Stan</td>\n",
       "      <td>You guys, you guys! Chef is going away. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Kyle</td>\n",
       "      <td>Going away? For how long?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Stan</td>\n",
       "      <td>Forever.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Chef</td>\n",
       "      <td>I'm sorry boys.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Stan</td>\n",
       "      <td>Chef said he's been bored, so he joining a gro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Season Episode Character                                               Line\n",
       "0     10       1      Stan         You guys, you guys! Chef is going away. \\n\n",
       "1     10       1      Kyle                        Going away? For how long?\\n\n",
       "2     10       1      Stan                                         Forever.\\n\n",
       "3     10       1      Chef                                  I'm sorry boys.\\n\n",
       "4     10       1      Stan  Chef said he's been bored, so he joining a gro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how raw daata looks like\n",
    "raw_data = pd.read_csv('./SouthParkData/All-seasons.csv')\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>Character</th>\n",
       "      <th>Line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>70896</td>\n",
       "      <td>70896</td>\n",
       "      <td>70896</td>\n",
       "      <td>70896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>3950</td>\n",
       "      <td>64301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>Cartman</td>\n",
       "      <td>What?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>6416</td>\n",
       "      <td>5271</td>\n",
       "      <td>9774</td>\n",
       "      <td>361</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Season Episode Character     Line\n",
       "count   70896   70896     70896    70896\n",
       "unique     19      19      3950    64301\n",
       "top         2      10   Cartman  What?\\n\n",
       "freq     6416    5271      9774      361"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#little summary, not all lines are unique. Moreover alot of characters\n",
    "raw_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Character\n",
       "Cartman         9774\n",
       "Stan            7680\n",
       "Kyle            7099\n",
       "Butters         2602\n",
       "Randy           2467\n",
       "Mr. Garrison    1002\n",
       "Chef             917\n",
       "Kenny            881\n",
       "Sharon           862\n",
       "Mr. Mackey       633\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count lines for character. Data have big \"tale\" of more or less unique characters\n",
    "#Мы берем в рассмортрение только 4х главных героев\n",
    "raw_data.groupby(['Character']).size().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Episode</th>\n",
       "      <th>Line</th>\n",
       "      <th>Season</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Character</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">Cartman</th>\n",
       "      <th>count</th>\n",
       "      <td>9774</td>\n",
       "      <td>9774</td>\n",
       "      <td>9774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>18</td>\n",
       "      <td>9340</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>7</td>\n",
       "      <td>What?\\n</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>850</td>\n",
       "      <td>52</td>\n",
       "      <td>801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">Kenny</th>\n",
       "      <th>count</th>\n",
       "      <td>881</td>\n",
       "      <td>881</td>\n",
       "      <td>881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>18</td>\n",
       "      <td>754</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>3</td>\n",
       "      <td>(Yeah!)\\n</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>106</td>\n",
       "      <td>17</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">Kyle</th>\n",
       "      <th>count</th>\n",
       "      <td>7099</td>\n",
       "      <td>7099</td>\n",
       "      <td>7099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>18</td>\n",
       "      <td>6493</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>1</td>\n",
       "      <td>What?\\n</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>635</td>\n",
       "      <td>61</td>\n",
       "      <td>824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">Stan</th>\n",
       "      <th>count</th>\n",
       "      <td>7680</td>\n",
       "      <td>7680</td>\n",
       "      <td>7680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>18</td>\n",
       "      <td>6995</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>10</td>\n",
       "      <td>What?\\n</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>594</td>\n",
       "      <td>73</td>\n",
       "      <td>831</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Episode       Line Season\n",
       "Character                                 \n",
       "Cartman   count     9774       9774   9774\n",
       "          unique      18       9340     18\n",
       "          top          7    What?\\n      4\n",
       "          freq       850         52    801\n",
       "Kenny     count      881        881    881\n",
       "          unique      18        754     18\n",
       "          top          3  (Yeah!)\\n      3\n",
       "          freq       106         17    132\n",
       "Kyle      count     7099       7099   7099\n",
       "          unique      18       6493     18\n",
       "          top          1    What?\\n      2\n",
       "          freq       635         61    824\n",
       "Stan      count     7680       7680   7680\n",
       "          unique      18       6995     18\n",
       "          top         10    What?\\n      2\n",
       "          freq       594         73    831"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#top_speakers = raw_data.groupby(['Character']).size(\n",
    "#    ).loc[raw_data.groupby(['Character']).size() > 2000]\n",
    "#print(top_speakers.index.values)\n",
    "# Выделяем реплики главных героев\n",
    "top_speakers = ['Stan', 'Kyle', 'Cartman', 'Kenny']\n",
    "main_char_lines = raw_data.loc[raw_data['Character'].isin(top_speakers)]\n",
    "main_char_lines.groupby(['Character']).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нормализация данных:\n",
    "лемматизаруем, убираем стоп-слова, но не добавляем обесуененую лексику,так как (кажется) некоторые персонажи выражаются чаще других\n",
    "Обязательно убираем всю пунктуацию и переносы строки, так как реплики Кенни в скобках"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ksenia/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "main_char_lines['Line'] = [line.replace('\\n','') for line in main_char_lines['Line']]\n",
    "train, evaluate = train_test_split(main_char_lines, test_size=0.3, random_state=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "разбиваем выборку на две части, вторую называем evaluate, на ней будем проверять модели в самом конце"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#preprocess data\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk import word_tokenize\n",
    "from string import punctuation\n",
    "\n",
    "st = LancasterStemmer()\n",
    "def token(text):\n",
    "    txt = nltk.word_tokenize(text.lower())\n",
    "    #print([st.stem(word) for word in txt if word not in punctuation])\n",
    "    return [st.stem(word) for word in txt if word not in punctuation]\n",
    "\n",
    "\n",
    "stop = set(stopwords.words(\"english\"))\n",
    "#cv = CountVectorizer(#lowercase=True, \n",
    "#                     tokenizer=token, #stop_words=stop,# token_pattern=u'(?u)\\b\\w\\w+\\b',\n",
    "#                     analyzer=u'word', min_df=4)\n",
    "cv = CountVectorizer(tokenizer=word_tokenize , stop_words=\"english\", max_features=5000)\n",
    "#print(train['Line'].tolist())\n",
    "\n",
    "train_vec = cv.fit_transform(train['Line'].tolist())\n",
    "evaluate_vec = cv.transform(evaluate['Line'].tolist())\n",
    "\n",
    "\n",
    "#print(vec_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#разбиваем трейн на трейн и тест еще раз, чтобы учиться на этом\n",
    "to_train_vec, to_test_vec, to_train_answ, to_test_answ = train_test_split(train_vec,\n",
    "                                                                            train['Character'], \n",
    "                                                                            test_size=0.3, \n",
    "                                                                            random_state=14)\n",
    "#print(to_train_vec[:2], to_train_answ[:2], to_test_vec[:2], to_test_answ[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    Cartman       0.37      0.37      0.37      2045\n",
      "      Kenny       0.02      0.03      0.03       176\n",
      "       Kyle       0.30      0.28      0.29      1509\n",
      "       Stan       0.29      0.30      0.30      1611\n",
      "\n",
      "avg / total       0.32      0.31      0.31      5341\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "dc = DummyClassifier()\n",
    "dc.fit(to_train_vec, to_train_answ)\n",
    "#print(to_test['Line'].tolist())\n",
    "print(classification_report(to_test_answ, dc.predict(to_test_vec), target_names=['Cartman', 'Kenny', 'Kyle' , 'Stan']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    Cartman       0.56      0.68      0.61      2045\n",
      "      Kenny       0.96      0.91      0.93       176\n",
      "       Kyle       0.46      0.30      0.37      1509\n",
      "       Stan       0.46      0.48      0.47      1611\n",
      "\n",
      "avg / total       0.51      0.52      0.51      5341\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
    "\n",
    "nb = MultinomialNB()\n",
    "#print(to_train_answ)\n",
    "nb.fit(to_train_vec, to_train_answ)\n",
    "\n",
    "print(classification_report(to_test_answ, nb.predict(to_test_vec), target_names=['Cartman', 'Kenny', 'Kyle' , 'Stan']))\n",
    "#accuracy_score(to_test_answ, nb.predict(to_test_vec),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    Cartman       0.59      0.68      0.63      2045\n",
      "      Kenny       0.99      0.95      0.97       176\n",
      "       Kyle       0.47      0.32      0.38      1509\n",
      "       Stan       0.46      0.51      0.48      1611\n",
      "\n",
      "avg / total       0.53      0.54      0.53      5341\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()#solver='sag', multi_class='multinomial', n_jobs=-1, max_iter=100)\n",
    "\n",
    "#lr = LogisticRegression(penalty=\"l2\", solver=\"lbfgs\", multi_class=\"multinomial\", max_iter=300, n_jobs=-1)\n",
    "lr.fit(to_train_vec.toarray(), to_train_answ)\n",
    "\n",
    "print(classification_report(to_test_answ, lr.predict(to_test_vec), target_names=['Cartman', 'Kenny', 'Kyle' , 'Stan']))\n",
    "#accuracy_score(to_test_answ, lr.predict(to_test_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    Cartman       0.56      0.66      0.60      2045\n",
      "      Kenny       0.99      0.95      0.97       176\n",
      "       Kyle       0.45      0.33      0.38      1509\n",
      "       Stan       0.44      0.44      0.44      1611\n",
      "\n",
      "avg / total       0.50      0.51      0.50      5341\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.53622917056730945"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1)\n",
    "rf.fit(to_train_vec.toarray(), to_train_answ)\n",
    "\n",
    "print(classification_report(to_test_answ, rf.predict(to_test_vec), target_names=['Cartman', 'Kenny', 'Kyle' , 'Stan']))\n",
    "accuracy_score(to_test_answ, lr.predict(to_test_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.458355111752\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    Cartman       0.56      0.66      0.60      2045\n",
      "      Kenny       0.99      0.95      0.97       176\n",
      "       Kyle       0.45      0.33      0.38      1509\n",
      "       Stan       0.44      0.44      0.44      1611\n",
      "\n",
      "avg / total       0.50      0.51      0.50      5341\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.53622917056730945"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "clf = AdaBoostClassifier(n_estimators=100)\n",
    "scores = cross_val_score(clf, to_train_vec.toarray(), to_train_answ)\n",
    "print(scores.mean())\n",
    "clf.fit(to_train_vec.toarray(), to_train_answ)\n",
    "print(classification_report(to_test_answ, rf.predict(to_test_vec), target_names=['Cartman', 'Kenny', 'Kyle' , 'Stan']))\n",
    "accuracy_score(to_test_answ, lr.predict(to_test_vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучше всего классифицирует логистическая регрессия, пыталась обучить multinominal, но не \"сошлась\" и точность поэтому не изменилась. Так же проеверяла на данных без удаления стоп-слов, качество ухудшилось, но незначительно. Этот код не привожу, чтобы не плодить уж совсем не читаемый документ.\n",
    "Длее проверим, улучшится ли точность, если применить TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    Cartman       0.55      0.75      0.64      2045\n",
      "      Kenny       0.99      0.94      0.96       176\n",
      "       Kyle       0.49      0.28      0.36      1509\n",
      "       Stan       0.47      0.45      0.46      1611\n",
      "\n",
      "avg / total       0.53      0.54      0.52      5341\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "stop = set(stopwords.words(\"english\"))\n",
    "#cv = CountVectorizer(#lowercase=True, \n",
    "#                     tokenizer=token, #stop_words=stop,# token_pattern=u'(?u)\\b\\w\\w+\\b',\n",
    "#                     analyzer=u'word', min_df=4)\n",
    "cv = TfidfVectorizer(tokenizer=word_tokenize , stop_words=\"english\", max_features=5000)\n",
    "#print(train['Line'].tolist())\n",
    "\n",
    "train_vec_tf = cv.fit_transform(train['Line'].tolist())\n",
    "evaluate_vec_tf = cv.transform(evaluate['Line'].tolist())\n",
    "\n",
    "tf_train_vec, tf_test_vec, tf_train_answ, tf_test_answ = train_test_split(train_vec_tf,\n",
    "                                                                            train['Character'], \n",
    "                                                                            test_size=0.3, \n",
    "                                                                            random_state=14)\n",
    "\n",
    "lr = LogisticRegression()#solver='sag', multi_class='multinomial', n_jobs=-1, max_iter=100)\n",
    "lr.fit(tf_train_vec.toarray(), tf_train_answ)\n",
    "\n",
    "print(classification_report(tf_test_answ, lr.predict(tf_test_vec), target_names=['Cartman', 'Kenny', 'Kyle' , 'Stan']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TfidfVectorizer показал себя практически так же, как и CounVectorizer, только немного ухудшил f-меру, поэтому дальше будем использовать первый вариант векторизации.\n",
    "Теперь посчитаем клучивые слова для каждого из главных героев через log-likelihood ratio.\n",
    "Для этого сначала выделим и токенезируем все фразы каждого героя. После этого посчитаем log-likelyhood вручную и выделим слова, специфичные для каждого говорящего (порог подобран вручную)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "import itertools\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    words = word_tokenize(text)\n",
    "    for word in words:\n",
    "        if word not in punctuation and word not in stop and word != '...':\n",
    "            result.append(word.lower())\n",
    "    return result\n",
    "\n",
    "#print(train.loc[train['Character']=='Cartman']['Line'].values)\n",
    "cartman_words = [preprocess(word) for word in train.loc[train['Character']=='Cartman']['Line'].values]\n",
    "kenny_words = [preprocess(word) for word in train.loc[train['Character']=='Kenny']['Line'].values]\n",
    "kyle_words = [preprocess(word) for word in train.loc[train['Character']=='Kyle']['Line'].values]\n",
    "stan_words = [preprocess(word) for word in train.loc[train['Character']=='Stan']['Line'].values]\n",
    "cartman_words = list(itertools.chain.from_iterable(cartman_words))\n",
    "kenny_words = list(itertools.chain.from_iterable(kenny_words))\n",
    "kyle_words = list(itertools.chain.from_iterable(kyle_words))\n",
    "stan_words = list(itertools.chain.from_iterable(stan_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ksenia/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:43: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/Ksenia/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:43: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#составление частотного списка\n",
    "def freq_dict(text):\n",
    "    #print(text)\n",
    "    f_dict = dict.fromkeys(set(text), 0)\n",
    "    \n",
    "    for word in text:\n",
    "        f_dict[word] += 1\n",
    "    #print(f_dict)\n",
    "    return f_dict\n",
    "\n",
    "#Функция, использующая предыдущие функции для выделения ключевых слов.\n",
    "def extract_keywords(text, others):\n",
    "    #print(text)\n",
    "    #others = list(itertools.chain.from_iterable(others))\n",
    "    #print(others)\n",
    "    #key_words = []\n",
    "    freq_text = freq_dict(text)\n",
    "    freq_others = freq_dict(others)\n",
    "    #print('mew')\n",
    "    columns = ['freq_txt','freq_others', 'other_txt',\n",
    "                             'other_others ', 'log_like']\n",
    "    #print('e' in set(text))\n",
    "    df_words = pd.DataFrame(columns=columns, index=set(text))\n",
    "    #print(df_words.index)\n",
    "    for word in freq_text:\n",
    "        #print(word)\n",
    "        #print(df_words.loc[word])\n",
    "\n",
    "        a = freq_text[word]\n",
    "        #print(a)\n",
    "        if word in freq_others.keys():\n",
    "            b = freq_others[word]\n",
    "        else:\n",
    "            b = 0\n",
    "        c = sum(freq_text.values())-a\n",
    "        d = sum(freq_others.values())-b\n",
    "        N = a+b+c+d\n",
    "        e1 = (a+c)*((a+b)/N)\n",
    "        e2 = (b+d)*((a+b)/N)\n",
    "        #print(a, b, c, d, e1, e2)\n",
    "        log_like = 2*((a*np.log(a/e1)+(b*np.log(b/e2))))\n",
    "        df_words.loc[word] = [a, b, c, d, log_like]\n",
    "    #print(df_words.sort_values('log_like', ascending=False).head(15))\n",
    "    \n",
    "    return df_words.loc[df_words['log_like'] >= 15.4265].index\n",
    "\n",
    "key_cartman = extract_keywords(cartman_words, kyle_words+kenny_words+stan_words)\n",
    "key_kyle = extract_keywords(kyle_words, cartman_words+kenny_words+stan_words)\n",
    "key_kenny = extract_keywords(kenny_words, kyle_words+cartman_words+stan_words)\n",
    "key_stan = extract_keywords(stan_words, kyle_words+kenny_words+cartman_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь надо сделать датасет, где будут учитываться ключевые слова. Для начала добавим в датафрейм столбцы для каждого ключевого слова и заполним единицами те строки, где это слово встречается (эта часть считается больше 30 минут)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Season Episode Character  \\\n",
      "0       10       1      Stan   \n",
      "1       10       1      Kyle   \n",
      "2       10       1      Stan   \n",
      "4       10       1      Stan   \n",
      "9       10       1   Cartman   \n",
      "10      10       1      Stan   \n",
      "19      10       1      Kyle   \n",
      "20      10       1   Cartman   \n",
      "21      10       1      Stan   \n",
      "23      10       1      Stan   \n",
      "24      10       1      Kyle   \n",
      "25      10       1   Cartman   \n",
      "26      10       1      Kyle   \n",
      "28      10       1      Stan   \n",
      "46      10       1      Kyle   \n",
      "48      10       1      Stan   \n",
      "49      10       1   Cartman   \n",
      "50      10       1      Stan   \n",
      "51      10       1      Kyle   \n",
      "52      10       1      Stan   \n",
      "53      10       1     Kenny   \n",
      "55      10       1   Cartman   \n",
      "56      10       1      Kyle   \n",
      "58      10       1      Stan   \n",
      "60      10       1      Kyle   \n",
      "62      10       1      Stan   \n",
      "64      10       1      Kyle   \n",
      "68      10       1      Kyle   \n",
      "70      10       1   Cartman   \n",
      "72      10       1      Kyle   \n",
      "74      10       1      Stan   \n",
      "76      10       1      Stan   \n",
      "78      10       1      Kyle   \n",
      "80      10       1      Stan   \n",
      "82      10       1      Stan   \n",
      "86      10       1      Stan   \n",
      "92      10       1      Kyle   \n",
      "95      10       1      Kyle   \n",
      "97      10       1      Stan   \n",
      "99      10       1      Kyle   \n",
      "101     10       1   Cartman   \n",
      "102     10       1      Kyle   \n",
      "103     10       1     Kenny   \n",
      "104     10       1      Kyle   \n",
      "105     10       1      Stan   \n",
      "106     10       1     Kenny   \n",
      "107     10       1   Cartman   \n",
      "108     10       1      Kyle   \n",
      "109     10       1   Cartman   \n",
      "111     10       1      Kyle   \n",
      "\n",
      "                                                  Line    i  jew  brother  \\\n",
      "0             You guys, you guys! Chef is going away.   0.0  0.0      0.0   \n",
      "1                            Going away? For how long?  0.0  0.0      0.0   \n",
      "2                                             Forever.  0.0  0.0      0.0   \n",
      "4    Chef said he's been bored, so he joining a gro...  0.0  0.0      0.0   \n",
      "9    I'm gonna miss him.  I'm gonna miss Chef and I...  1.0  0.0      0.0   \n",
      "10   Dude, how are we gonna go on? Chef was our fuh...  0.0  0.0      0.0   \n",
      "19                              Draw two card, fatass.  0.0  0.0      0.0   \n",
      "20                               Reverse to you, Jew.   0.0  1.0      0.0   \n",
      "21                                       I'll get it.   1.0  0.0      0.0   \n",
      "23                                          He's back!  0.0  0.0      0.0   \n",
      "24                                               Yeah!  0.0  0.0      0.0   \n",
      "25                                         All right!   0.0  0.0      0.0   \n",
      "26                  Chef! I can't believe you're back!  1.0  0.0      0.0   \n",
      "28                          But are you back for good?  0.0  0.0      0.0   \n",
      "46   Well, I- guess we'll see you in school tomorro...  0.0  0.0      0.0   \n",
      "48                                  Right. Uh, see ya.  0.0  0.0      0.0   \n",
      "49   Uh, guys? Did Chef seem a little, uh, trippy t...  0.0  0.0      0.0   \n",
      "50   Well, look. he said he's happier now. Maybe he...  0.0  0.0      0.0   \n",
      "51   Yeah. I'm sure whatever that Super Adventure C...  1.0  0.0      0.0   \n",
      "52   Yeah, but whatever, I'm just glad he's back fo...  1.0  0.0      0.0   \n",
      "53                                     (Yeah, me too.)  0.0  0.0      0.0   \n",
      "55   Oh boy oh boy, I can't wait to have Chef's lun...  1.0  0.0      0.0   \n",
      "56   Yeah. I hope he makes his Salisbury steak with...  1.0  0.0      0.0   \n",
      "58                                               What?  0.0  0.0      0.0   \n",
      "60                                          Like what?  0.0  0.0      0.0   \n",
      "62                                              What??  0.0  0.0      0.0   \n",
      "64                                            Weirdo.   0.0  0.0      0.0   \n",
      "68                                               Good.  0.0  0.0      0.0   \n",
      "70                                          Excuse me?  0.0  0.0      0.0   \n",
      "72                           ...Chef?? A-are you okay?  0.0  0.0      0.0   \n",
      "74                         Dude, what are you saying??  0.0  0.0      0.0   \n",
      "76                                           ...WHAT??  0.0  0.0      0.0   \n",
      "78                        This doesn't make any sense!  0.0  0.0      0.0   \n",
      "80                                       No, he's not.  0.0  0.0      0.0   \n",
      "82                                       No, he's not.  0.0  0.0      0.0   \n",
      "86                                                 NO!  0.0  0.0      0.0   \n",
      "92   Goddammit, Chef isn't like that! Something fun...  0.0  0.0      0.0   \n",
      "95    Chef, the police are asking questions about you!  0.0  0.0      0.0   \n",
      "97          No, Chef, we don't wanna make love to you!  0.0  0.0      0.0   \n",
      "99   Chef, CHEF! You need to get out of here before...  0.0  0.0      0.0   \n",
      "101  ...Man, I can't believe all this time, Chef ju...  1.0  0.0      0.0   \n",
      "102  He didn't want us for sex, fatass! Something i...  0.0  0.0      0.0   \n",
      "103                                       (Like what?)  0.0  0.0      0.0   \n",
      "104  Something must have happened to Chef while he ...  0.0  0.0      0.0   \n",
      "105  Well look: he spent the last three months with...  0.0  0.0      0.0   \n",
      "106                                 (Yeah! I think...)  1.0  0.0      0.0   \n",
      "107                                              Yeah!  0.0  0.0      0.0   \n",
      "108                           All right, come on guys!  0.0  0.0      0.0   \n",
      "109  Hey you guys, you know what they call a Jewish...  0.0  0.0      0.0   \n",
      "111  Ahh, hi, can we speak to the head guy or somet...  0.0  0.0      0.0   \n",
      "\n",
      "     chicken  but  something  ...    bullshit  gorak  fifth  evil  shelly  \\\n",
      "0        0.0  0.0        0.0  ...         0.0    0.0    0.0   0.0     0.0   \n",
      "1        0.0  0.0        0.0  ...         0.0    0.0    0.0   0.0     0.0   \n",
      "2        0.0  0.0        0.0  ...         0.0    0.0    0.0   0.0     0.0   \n",
      "4        0.0  0.0        0.0  ...         0.0    0.0    0.0   0.0     0.0   \n",
      "9        0.0  0.0        0.0  ...         0.0    0.0    0.0   0.0     0.0   \n",
      "10       0.0  0.0        0.0  ...         0.0    0.0    0.0   0.0     0.0   \n",
      "19       0.0  0.0        0.0  ...         0.0    0.0    0.0   0.0     0.0   \n",
      "20       0.0  0.0        0.0  ...         0.0    0.0    0.0   0.0     0.0   \n",
      "21       0.0  0.0        0.0  ...         0.0    0.0    0.0   0.0     0.0   \n",
      "23       0.0  0.0        0.0  ...         0.0    0.0    0.0   0.0     0.0   \n",
      "24       0.0  0.0        0.0  ...         0.0    0.0    0.0   0.0     0.0   \n",
      "25       0.0  0.0        0.0  ...         0.0    0.0    0.0   0.0     0.0   \n",
      "26       0.0  0.0        0.0  ...         0.0    0.0    0.0   0.0     0.0   \n",
      "28       0.0  1.0        0.0  ...         0.0    0.0    0.0   0.0     0.0   \n",
      "46       0.0  0.0        0.0  ...         0.0    0.0    0.0   0.0     0.0   \n",
      "48       0.0  0.0        0.0  ...         0.0    0.0    0.0   0.0     0.0   \n",
      "49       0.0  0.0        0.0  ...         0.0    0.0    0.0   0.0     0.0   \n",
      "50       0.0  0.0        0.0  ...         0.0    0.0    0.0   0.0     0.0   \n",
      "51       0.0  0.0        0.0  ...         0.0    0.0    0.0   0.0     0.0   \n",
      "52       0.0  0.0        0.0  ...         0.0    0.0    0.0   0.0     0.0   \n",
      "53       0.0  0.0        0.0  ...         0.0    0.0    0.0   0.0     0.0   \n",
      "55       0.0  0.0        0.0  ...         0.0    0.0    0.0   0.0     0.0   \n",
      "56       0.0  0.0        0.0  ...         0.0    0.0    0.0   0.0     0.0   \n",
      "58       0.0  0.0        0.0  ...         0.0    0.0    0.0   0.0     0.0   \n",
      "60       0.0  0.0        0.0  ...         0.0    0.0    0.0   0.0     0.0   \n",
      "62       0.0  0.0        0.0  ...         0.0    0.0    0.0   0.0     0.0   \n",
      "64       0.0  0.0        0.0  ...         0.0    0.0    0.0   0.0     0.0   \n",
      "68       0.0  0.0        0.0  ...         0.0    0.0    0.0   0.0     0.0   \n",
      "70       0.0  0.0        0.0  ...         0.0    0.0    0.0   0.0     0.0   \n",
      "72       0.0  0.0        0.0  ...         0.0    0.0    0.0   0.0     0.0   \n",
      "74       0.0  0.0        0.0  ...         0.0    0.0    0.0   0.0     0.0   \n",
      "76       0.0  0.0        0.0  ...         0.0    0.0    0.0   0.0     0.0   \n",
      "78       0.0  0.0        0.0  ...         0.0    0.0    0.0   0.0     0.0   \n",
      "80       0.0  0.0        0.0  ...         0.0    0.0    0.0   0.0     0.0   \n",
      "82       0.0  0.0        0.0  ...         0.0    0.0    0.0   0.0     0.0   \n",
      "86       0.0  0.0        0.0  ...         0.0    0.0    0.0   0.0     0.0   \n",
      "92       0.0  0.0        1.0  ...         0.0    0.0    0.0   0.0     0.0   \n",
      "95       0.0  0.0        0.0  ...         0.0    0.0    0.0   0.0     0.0   \n",
      "97       0.0  0.0        0.0  ...         0.0    0.0    0.0   0.0     0.0   \n",
      "99       0.0  0.0        0.0  ...         0.0    0.0    0.0   0.0     0.0   \n",
      "101      0.0  0.0        0.0  ...         0.0    0.0    0.0   0.0     0.0   \n",
      "102      0.0  0.0        1.0  ...         0.0    0.0    0.0   0.0     0.0   \n",
      "103      0.0  0.0        0.0  ...         0.0    0.0    0.0   0.0     0.0   \n",
      "104      0.0  0.0        1.0  ...         0.0    0.0    0.0   0.0     0.0   \n",
      "105      0.0  0.0        0.0  ...         0.0    0.0    0.0   0.0     0.0   \n",
      "106      0.0  0.0        0.0  ...         0.0    0.0    0.0   0.0     0.0   \n",
      "107      0.0  0.0        0.0  ...         0.0    0.0    0.0   0.0     0.0   \n",
      "108      0.0  0.0        0.0  ...         0.0    0.0    0.0   0.0     0.0   \n",
      "109      0.0  0.0        0.0  ...         0.0    0.0    0.0   0.0     0.0   \n",
      "111      0.0  0.0        1.0  ...         0.0    0.0    0.0   0.0     0.0   \n",
      "\n",
      "     come   's  profile  grampa  jesus  \n",
      "0     0.0  0.0      0.0     0.0    0.0  \n",
      "1     0.0  0.0      0.0     0.0    0.0  \n",
      "2     0.0  0.0      0.0     0.0    0.0  \n",
      "4     0.0  1.0      0.0     0.0    0.0  \n",
      "9     0.0  0.0      0.0     0.0    0.0  \n",
      "10    0.0  0.0      0.0     0.0    0.0  \n",
      "19    0.0  0.0      0.0     0.0    0.0  \n",
      "20    0.0  0.0      0.0     0.0    0.0  \n",
      "21    0.0  0.0      0.0     0.0    0.0  \n",
      "23    0.0  1.0      0.0     0.0    0.0  \n",
      "24    0.0  0.0      0.0     0.0    0.0  \n",
      "25    0.0  0.0      0.0     0.0    0.0  \n",
      "26    0.0  0.0      0.0     0.0    0.0  \n",
      "28    0.0  0.0      0.0     0.0    0.0  \n",
      "46    0.0  0.0      0.0     0.0    0.0  \n",
      "48    0.0  0.0      0.0     0.0    0.0  \n",
      "49    0.0  0.0      0.0     0.0    0.0  \n",
      "50    0.0  1.0      0.0     0.0    0.0  \n",
      "51    0.0  0.0      0.0     0.0    0.0  \n",
      "52    0.0  1.0      0.0     0.0    0.0  \n",
      "53    0.0  0.0      0.0     0.0    0.0  \n",
      "55    0.0  1.0      0.0     0.0    0.0  \n",
      "56    0.0  0.0      0.0     0.0    0.0  \n",
      "58    0.0  0.0      0.0     0.0    0.0  \n",
      "60    0.0  0.0      0.0     0.0    0.0  \n",
      "62    0.0  0.0      0.0     0.0    0.0  \n",
      "64    0.0  0.0      0.0     0.0    0.0  \n",
      "68    0.0  0.0      0.0     0.0    0.0  \n",
      "70    0.0  0.0      0.0     0.0    0.0  \n",
      "72    0.0  0.0      0.0     0.0    0.0  \n",
      "74    0.0  0.0      0.0     0.0    0.0  \n",
      "76    0.0  0.0      0.0     0.0    0.0  \n",
      "78    0.0  0.0      0.0     0.0    0.0  \n",
      "80    0.0  1.0      0.0     0.0    0.0  \n",
      "82    0.0  1.0      0.0     0.0    0.0  \n",
      "86    0.0  0.0      0.0     0.0    0.0  \n",
      "92    0.0  0.0      0.0     0.0    0.0  \n",
      "95    0.0  0.0      0.0     0.0    0.0  \n",
      "97    0.0  0.0      0.0     0.0    0.0  \n",
      "99    0.0  0.0      0.0     0.0    0.0  \n",
      "101   0.0  0.0      0.0     0.0    0.0  \n",
      "102   0.0  0.0      0.0     0.0    0.0  \n",
      "103   0.0  0.0      0.0     0.0    0.0  \n",
      "104   0.0  0.0      0.0     0.0    0.0  \n",
      "105   0.0  0.0      0.0     0.0    0.0  \n",
      "106   0.0  0.0      0.0     0.0    0.0  \n",
      "107   0.0  0.0      0.0     0.0    0.0  \n",
      "108   1.0  0.0      0.0     0.0    0.0  \n",
      "109   0.0  1.0      0.0     0.0    0.0  \n",
      "111   0.0  0.0      0.0     0.0    0.0  \n",
      "\n",
      "[50 rows x 112 columns]\n"
     ]
    }
   ],
   "source": [
    "data_with_keys = main_char_lines.copy()\n",
    "#print(key_cartman)\n",
    "all_keys = list(itertools.chain.from_iterable([key_cartman, key_kyle, key_kenny, key_stan]))\n",
    "#print(all_keys)\n",
    "#data_with_keys.add(all_keys, axis='columns', fill_value=0)\n",
    "percent = 0\n",
    "for key in all_keys:\n",
    "    #kk = len(all_keys) - \n",
    "    data_with_keys[key] = pd.Series()\n",
    "    for index, line in data_with_keys.iterrows():\n",
    "        #print(line)\n",
    "        if key in set(preprocess(line['Line'])):\n",
    "            data_with_keys.set_value(index, key, 1)\n",
    "        else:\n",
    "            data_with_keys.set_value(index, key, 0)\n",
    "        #pr = \n",
    "\n",
    "print(data_with_keys.head(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я понимаю, что лучше бы было счиать только для теста, однако пересчитывать не буду, так как очень долго. разобьем заново и посчитаем логит регрессю."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    Cartman       0.49      0.80      0.60      2045\n",
      "      Kenny       0.36      0.02      0.04       176\n",
      "       Kyle       0.56      0.20      0.30      1509\n",
      "       Stan       0.43      0.38      0.40      1611\n",
      "\n",
      "avg / total       0.49      0.48      0.44      5341\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_keys, eval_keys = train_test_split(data_with_keys, \n",
    "                                        test_size=0.3, \n",
    "                                        random_state=14)\n",
    "tr_keys, ts_keys = train_test_split(train_keys, \n",
    "                                        test_size=0.3, \n",
    "                                        random_state=14)\n",
    "lr = LogisticRegression()#solver='sag', multi_class='multinomial', n_jobs=-1, max_iter=100)\n",
    "lr.fit(tr_keys.ix[:,4:], tr_keys['Character'])\n",
    "\n",
    "print(classification_report(ts_keys['Character'], lr.predict(ts_keys.ix[:,4:]), target_names=['Cartman', 'Kenny', 'Kyle' , 'Stan']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат значительно хуже, чем по веркторам.\n",
    "Теперь подберем оптимальные параметры LogRegression через gridsearch (до этого все параметры подбирались вручную)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class_weight': None, 'max_iter': 150, 'penalty': 'l1'} 0.525517573423\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    Cartman       0.58      0.70      0.63      2045\n",
      "      Kenny       0.99      0.95      0.97       176\n",
      "       Kyle       0.50      0.29      0.37      1509\n",
      "       Stan       0.46      0.52      0.49      1611\n",
      "\n",
      "avg / total       0.53      0.54      0.53      5341\n",
      "\n",
      "----------------->    final score   <-----------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    Cartman       0.60      0.67      0.63      2984\n",
      "      Kenny       0.96      0.96      0.96       249\n",
      "       Kyle       0.49      0.31      0.38      2145\n",
      "       Stan       0.42      0.51      0.46      2253\n",
      "\n",
      "avg / total       0.53      0.53      0.52      7631\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "lr = LogisticRegression()#solver='sag', multi_class='multinomial', n_jobs=-1, max_iter=100)\n",
    "\n",
    "params = [{'penalty': ['l1','l2'], 'class_weight':[None,'balanced'], 'max_iter':[100, 150, 250]}]\n",
    "clf = GridSearchCV(lr, params)\n",
    "clf.fit(to_train_vec.toarray(), to_train_answ)\n",
    "print(clf.best_params_, clf.best_score_)\n",
    "\n",
    "#lr_top = LogisticRegression(clf.best_params_)\n",
    "#lr_top.fit(tf_train_vec.toarray(), tf_train_answ)\n",
    "print(classification_report(to_test_answ, clf.predict(to_test_vec), target_names=['Cartman', 'Kenny', 'Kyle' , 'Stan']))\n",
    "\n",
    "#и заоодно проверим лучший классификатор на evaluate, так как больше мы его улучшать не будем\n",
    "print('----------------->    final score   <-----------------')\n",
    "print(classification_report(evaluate['Character'], clf.predict(evaluate_vec), target_names=['Cartman', 'Kenny', 'Kyle' , 'Stan']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что на проверочной выборке показатели немного хуже, но, в целом, результат хороший.\n",
    "Теперь построим матрицу ошибок для лучшей модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVsAAAEmCAYAAADMczPyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBfDALzSsRAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x136427908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "mat = confusion_matrix(to_train_answ, clf.predict(to_train_vec.toarray()))\n",
    "plt.figure()\n",
    "plot_confusion_matrix(mat, classes=['Cartman', 'Kenny' , 'Kyle' , 'Stan'],\n",
    "                      title='Confusion matrix. Logistic Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В принципе, получившаяся модель на 20-22% более эффективна, чем beseline. Если посмотреть на коэффициенты модели, то они могут быть положительными и отрицатемльными. В логистической регрессии коэффициенты имеют очень наглядный физический смысл:\n",
    "в матрице, где каждое слово может встретиться в предложении (1), а может не встретиться (0), это число (1 или 0) уменожается на коэффициент. То сеть каждое встретившееся слово в предложении привносит (или отнимает)  свою долю от итогового счета.\n",
    "Коэффициенты есть для каждого героя."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
